{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import logging\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('scraping.log'),\n",
    "        logging.StreamHandler()])\n",
    "logger = logging.getLogger(__name__)\n",
    "class IndianKanoonScraper:\n",
    "    def __init__(self, base_dir: str = \"dataset_raw\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_url = \"https://indiankanoon.org\"\n",
    "        self.search_configs = {}        \n",
    "        courts = {\n",
    "            \"supreme_court\": {\n",
    "                \"url_key\": \"supremecourt\", \"prefix\": \"SC\",\n",
    "                \"years\": range(2000, 2025)},\n",
    "            \"delhi_high_court\": {\n",
    "                \"url_key\": \"delhi\", \"prefix\": \"DHC\",\n",
    "                \"years\": range(2000, 2025)},\n",
    "            \"bombay_high_court\": {\n",
    "                \"url_key\": \"bombay\", \"prefix\": \"BHC\",\n",
    "                \"years\": range(2000, 2025)},\n",
    "            \"calcutta_high_court\": {\n",
    "                \"url_key\": \"kolkata\", \"prefix\": \"CHC\",\n",
    "                \"years\": [y for y in range(2000, 2025) if y != 2012]},\n",
    "            \"allahabad_high_court\": {\n",
    "                \"url_key\": \"allahabad\", \"prefix\": \"AHC\",\n",
    "                \"years\": [y for y in range(2000, 2025) if y != 2009]},\n",
    "            \"madras_high_court\": {\n",
    "                \"url_key\": \"chennai\", \"prefix\": \"MHC\",\n",
    "                \"years\": range(2000, 2025)}}        \n",
    "        for court_folder, config in courts.items():\n",
    "            for year in config[\"years\"]:\n",
    "                key = f\"{court_folder}_{year}\"\n",
    "                self.search_configs[key] = {\n",
    "                    \"url\": f\"https://indiankanoon.org/search/?formInput=doctypes:{config['url_key']}%20year:{year}\",\n",
    "                    \"folder\": court_folder, \"prefix\": f\"{config['prefix']}_{year}\"}\n",
    "        self.setup_directories()        \n",
    "    def setup_directories(self):\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "        (self.base_dir / \"supreme_court\").mkdir(exist_ok=True)\n",
    "        (self.base_dir / \"delhi_high_court\").mkdir(exist_ok=True)\n",
    "        (self.base_dir / \"bombay_high_court\").mkdir(exist_ok=True)\n",
    "        (self.base_dir / \"calcutta_high_court\").mkdir(exist_ok=True)\n",
    "        (self.base_dir / \"allahabad_high_court\").mkdir(exist_ok=True)\n",
    "        (self.base_dir / \"madras_high_court\").mkdir(exist_ok=True)\n",
    "        logger.info(f\"Created processed directory structure in {self.base_dir}\")\n",
    "    def init_driver(self):\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument('--no-sandbox')\n",
    "        options.add_argument('--disable-dev-shm-usage')\n",
    "        options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "        options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')        \n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        driver = webdriver.Chrome(service=service, options=options)\n",
    "        return driver\n",
    "    def get_case_links_from_page(self, driver) -> List[str]:\n",
    "        links = []\n",
    "        try:\n",
    "            time.sleep(2)\n",
    "            try:\n",
    "                results = driver.find_elements(By.CLASS_NAME, \"result_title\")\n",
    "                for result in results:\n",
    "                    try:\n",
    "                        link = result.find_element(By.TAG_NAME, \"a\").get_attribute(\"href\")\n",
    "                        if link and '/doc/' in link:\n",
    "                            links.append(link)\n",
    "                    except:\n",
    "                        continue\n",
    "            except:\n",
    "                pass            \n",
    "            if not links:\n",
    "                all_links = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "                for element in all_links:\n",
    "                    href = element.get_attribute(\"href\")\n",
    "                    if href and '/doc/' in href and 'indiankanoon.org/doc/' in href:\n",
    "                        if href not in links:\n",
    "                            links.append(href)            \n",
    "            if not links:\n",
    "                logger.warning(\"No links found on page. Saving page source for debugging.\")\n",
    "                with open(\"debug_page.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                    f.write(driver.page_source)\n",
    "                logger.warning(\"Page source saved to debug_page.html\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting links from page: {e}\")\n",
    "        return links\n",
    "    def click_next_page(self, driver) -> bool:\n",
    "        try:\n",
    "            time.sleep(2)\n",
    "            try:\n",
    "                next_links = driver.find_elements(By.XPATH, \"//a[contains(text(), 'Next') or contains(text(), 'next')]\")\n",
    "                if next_links:\n",
    "                    next_links[0].click()\n",
    "                    time.sleep(3)\n",
    "                    return True\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                next_links = driver.find_elements(By.XPATH, \"//a[contains(text(), '›') or contains(text(), '→')]\")\n",
    "                if next_links:\n",
    "                    next_links[0].click()\n",
    "                    time.sleep(3)\n",
    "                    return True\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                page_links = driver.find_elements(By.CSS_SELECTOR, \"div.browse a, .pagination a\")\n",
    "                current_url = driver.current_url\n",
    "                for link in page_links:\n",
    "                    link_text = link.text.strip()\n",
    "                    if 'next' in link_text.lower() or '›' in link_text:\n",
    "                        link.click()\n",
    "                        time.sleep(3)\n",
    "                        if driver.current_url != current_url:\n",
    "                            return True\n",
    "            except:\n",
    "                pass\n",
    "            logger.warning(\"Could not find 'Next' button\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error clicking next page: {e}\")\n",
    "            return False\n",
    "    def download_case_html(self, case_url: str, retry_count: int = 3) -> Optional[str]:\n",
    "        for attempt in range(retry_count):\n",
    "            try:\n",
    "                response = requests.get(case_url, timeout=30)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except requests.exceptions.HTTPError as e:\n",
    "                if e.response.status_code == 429:  \n",
    "                    wait_time = (attempt + 1) * 5 \n",
    "                    logger.warning(f\"Rate limited. Waiting {wait_time}s before retry {attempt + 1}/{retry_count}\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    logger.error(f\"HTTP Error downloading {case_url}: {e}\")\n",
    "                    return None\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error downloading {case_url}: {e}\")\n",
    "                return None\n",
    "        logger.error(f\"Failed to download {case_url} after {retry_count} retries\")\n",
    "        return None\n",
    "    def scrape_court_year(self, config_key: str, max_cases: int = 1000):\n",
    "        config = self.search_configs[config_key]\n",
    "        logger.info(f\"Starting scraping for {config_key}\")\n",
    "        driver = self.init_driver()\n",
    "        case_links = []\n",
    "        try:\n",
    "            driver.get(config['url'])\n",
    "            logger.info(f\"Navigated to {config['url']}\")\n",
    "            time.sleep(5) \n",
    "            page_num = 1\n",
    "            while len(case_links) < max_cases:\n",
    "                logger.info(f\"Scraping page {page_num} for {config_key}\")\n",
    "                links = self.get_case_links_from_page(driver)\n",
    "                case_links.extend(links)\n",
    "                logger.info(f\"Found {len(links)} cases on page {page_num}. Total: {len(case_links)}\")\n",
    "                if page_num == 1 and len(links) == 0:\n",
    "                    logger.error(f\"No results found on first page for {config_key}. Check debug_page.html\")\n",
    "                    break\n",
    "                if len(case_links) >= max_cases:\n",
    "                    case_links = case_links[:max_cases]\n",
    "                    break\n",
    "                if not self.click_next_page(driver):\n",
    "                    logger.info(f\"No more pages available for {config_key}\")\n",
    "                    break\n",
    "                page_num += 1\n",
    "                time.sleep(3) \n",
    "        finally:\n",
    "            driver.quit()        \n",
    "        logger.info(f\"Downloading {len(case_links)} cases for {config_key}\")\n",
    "        successful_downloads = 0\n",
    "        for idx, link in enumerate(case_links, 1):\n",
    "            html_content = self.download_case_html(link)\n",
    "            if html_content:\n",
    "                filename = f\"{config['prefix']}_{idx:04d}.html\"\n",
    "                filepath = self.base_dir / config['folder'] / filename\n",
    "                with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                    f.write(html_content)\n",
    "                successful_downloads += 1\n",
    "                logger.info(f\"Saved {filename} ({successful_downloads}/{len(case_links)})\")\n",
    "            time.sleep(2) \n",
    "        logger.info(f\"Completed scraping {config_key}: {successful_downloads}/{len(case_links)} cases downloaded successfully\")\n",
    "        return successful_downloads\n",
    "class LegalCasePreprocessor:    \n",
    "    def __init__(self, raw_dir: str = \"dataset_raw\", processed_dir: str = \"dataset_processed\"):\n",
    "        self.raw_dir = Path(raw_dir)\n",
    "        self.processed_dir = Path(processed_dir)\n",
    "        self.setup_directories()\n",
    "        self.citation_pattern = r'/doc/\\d+'    \n",
    "    def setup_directories(self):\n",
    "        self.processed_dir.mkdir(exist_ok=True)\n",
    "        (self.processed_dir / \"supreme_court\").mkdir(exist_ok=True)\n",
    "        (self.processed_dir / \"delhi_high_court\").mkdir(exist_ok=True)\n",
    "        (self.processed_dir / \"bombay_high_court\").mkdir(exist_ok=True)\n",
    "        (self.processed_dir / \"calcutta_high_court\").mkdir(exist_ok=True)\n",
    "        (self.processed_dir / \"allahabad_high_court\").mkdir(exist_ok=True)\n",
    "        (self.processed_dir / \"madras_high_court\").mkdir(exist_ok=True)\n",
    "        logger.info(f\"Created processed directory structure in {self.processed_dir}\")    \n",
    "    def extract_metadata(self, soup: BeautifulSoup, html_content: str) -> Dict:\n",
    "        metadata = {\n",
    "            'title': '', 'court': '', 'date': '', \n",
    "            'citations': [], 'petitioner': '','respondent': ''}        \n",
    "        title_elem = soup.find('h1', class_='docsource_main')\n",
    "        if not title_elem:\n",
    "            title_elem = soup.find('title')\n",
    "        if not title_elem:\n",
    "            title_patterns = [r'([A-Z][A-Za-z\\s&,\\.]+)\\s+(?:vs?\\.?|versus)\\s+([A-Z][A-Za-z\\s&,\\.]+)\\s+on\\s+\\d{1,2}\\s+\\w+,?\\s+\\d{4}',]\n",
    "            for pattern in title_patterns:\n",
    "                match = re.search(pattern, html_content[:2000])\n",
    "                if match:\n",
    "                    metadata['title'] = match.group(0)\n",
    "                    break\n",
    "        if title_elem and not metadata['title']:\n",
    "            metadata['title'] = title_elem.get_text(strip=True)        \n",
    "        filename = soup.find('title')\n",
    "        if filename:\n",
    "            filename_text = filename.get_text()\n",
    "            if 'Delhi High Court' in filename_text:\n",
    "                metadata['court'] = 'Delhi High Court'\n",
    "            elif 'Supreme Court' in filename_text:\n",
    "                metadata['court'] = 'Supreme Court of India'\n",
    "        if not metadata['court']:\n",
    "            court_patterns = [\n",
    "                (r'IN\\s+THE\\s+HIGH\\s+COURT\\s+OF\\s+DELHI\\s+AT\\s+NEW\\s+DELHI', 'Delhi High Court'),\n",
    "                (r'HIGH\\s+COURT\\s+OF\\s+DELHI\\s+AT\\s+NEW\\s+DELHI', 'Delhi High Court'),\n",
    "                (r'DELHI\\s+HIGH\\s+COURT', 'Delhi High Court'),\n",
    "                (r'IN\\s+THE\\s+SUPREME\\s+COURT\\s+OF\\s+INDIA', 'Supreme Court of India'),\n",
    "                (r'SUPREME\\s+COURT\\s+OF\\s+INDIA', 'Supreme Court of India'),]\n",
    "            search_text = html_content[:2000]\n",
    "            for pattern, court_name in court_patterns:\n",
    "                match = re.search(pattern, search_text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    metadata['court'] = court_name\n",
    "                    break\n",
    "        if metadata['title']:\n",
    "            date_in_title = re.search(r'on\\s+(\\d{1,2}\\s+\\w+,?\\s+\\d{4})', metadata['title'])\n",
    "            if date_in_title:\n",
    "                date_str = date_in_title.group(1).replace(',', '')\n",
    "                metadata['date'] = self.normalize_date(date_str)        \n",
    "        if not metadata['date']:\n",
    "            date_patterns = [\n",
    "                r'(?:Decided\\s+On|Decided|Date\\s+of\\s+Judgment|Judgment\\s+Date):\\s*(\\d{1,2}[/-]\\d{1,2}[/-]\\d{4})',\n",
    "                r'(?:Decided\\s+On|Decided|Date\\s+of\\s+Judgment):\\s*(\\d{1,2}\\s+\\w+,?\\s+\\d{4})',\n",
    "                r'(\\d{1,2}\\s+(?:January|February|March|April|May|June|July|August|September|October|November|December),?\\s+\\d{4})']\n",
    "            for pattern in date_patterns:\n",
    "                match = re.search(pattern, html_content[:3000], re.IGNORECASE)\n",
    "                if match:\n",
    "                    metadata['date'] = self.normalize_date(match.group(1))\n",
    "                    break        \n",
    "        citation_elements = soup.find_all('a', href=re.compile(r'/doc/\\d+'))\n",
    "        seen_citations = set()\n",
    "        for elem in citation_elements:\n",
    "            citation = elem.get_text(strip=True)\n",
    "            if (citation and \n",
    "                citation not in seen_citations and \n",
    "                len(citation) > 15 and\n",
    "                len(citation) < 200 and\n",
    "                not citation.startswith('Section') and\n",
    "                not citation.startswith('Article') and\n",
    "                'vs' in citation.lower() or 'v.' in citation.lower()):\n",
    "                metadata['citations'].append(citation)\n",
    "                seen_citations.add(citation)\n",
    "                if len(metadata['citations']) >= 20: \n",
    "                    break        \n",
    "        if metadata['title']:\n",
    "            vs_pattern = r'^(.+?)\\s+(?:vs?\\.?|versus)\\s+(.+?)\\s+on\\s+\\d'\n",
    "            match = re.search(vs_pattern, metadata['title'], re.IGNORECASE)\n",
    "            if match:\n",
    "                metadata['petitioner'] = match.group(1).strip()\n",
    "                metadata['respondent'] = match.group(2).strip()\n",
    "        return metadata    \n",
    "    def normalize_date(self, date_str: str) -> str:\n",
    "        try:\n",
    "            for fmt in ['%d %B %Y', '%d-%m-%Y', '%d/%m/%Y', '%Y-%m-%d', '%Y/%m/%d']:\n",
    "                try:\n",
    "                    dt = datetime.strptime(date_str, fmt)\n",
    "                    return dt.strftime('%Y-%m-%d')\n",
    "                except ValueError:\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not parse date: {date_str}\")\n",
    "        return date_str    \n",
    "    def clean_text(self, soup: BeautifulSoup) -> str:\n",
    "        for script in soup(['script', 'style', 'header', 'footer', 'nav']):\n",
    "            script.decompose()        \n",
    "        for promo in soup.find_all(['div', 'p'], class_=['ad', 'promo', 'advertisement']):\n",
    "            promo.decompose()\n",
    "        main_content = soup.find('div', class_='judgments')\n",
    "        if not main_content:\n",
    "            main_content = soup.find('div', id='div1')\n",
    "        if not main_content:\n",
    "            main_content = soup.find('body')\n",
    "        if main_content:\n",
    "            text = main_content.get_text(separator='\\n')\n",
    "        else:\n",
    "            text = soup.get_text(separator='\\n')        \n",
    "        lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "        text = '\\n'.join(lines)\n",
    "        promo_start_patterns = [\n",
    "            r'^Take notes as you read a judgment using our.*?free trial for one month\\.',\n",
    "            r'^Take notes as you read a judgment.*?one month\\.',]\n",
    "        for pattern in promo_start_patterns:\n",
    "            text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE)        \n",
    "        promo_patterns = [\n",
    "            r'Take notes as you read a judgment.*?Try out our Premium Member Services.*?one month\\.',\n",
    "            r'Virtual Legal Assistant.*?Query Alert Service.*?Premium Member Services', r'Try out our Premium Member.*?free trial.*?one month',\n",
    "            r'Sign up today and get free trial for one month', r'Premium Member Services\\s*--\\s*Sign up today',\n",
    "            r'Print Page.*?Email Page', r'Cite.*?Print.*?Email',]\n",
    "        for pattern in promo_patterns:\n",
    "            text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE)\n",
    "        text = re.sub(r'\\n\\s*\\n\\s*\\n+', '\\n\\n', text)\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        text = text.strip()        \n",
    "        return text    \n",
    "    def process_case(self, html_path: Path) -> Optional[Dict]:\n",
    "        try:\n",
    "            with open(html_path, 'r', encoding='utf-8') as f:\n",
    "                html_content = f.read()\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')            \n",
    "            court_folder = html_path.parent.name\n",
    "            if court_folder == 'supreme_court':\n",
    "                court_name = 'Supreme Court of India'\n",
    "            elif court_folder == 'delhi_high_court':\n",
    "                court_name = 'Delhi High Court'\n",
    "            elif court_folder == 'bombay_high_court':\n",
    "                court_name = 'Bombay High Court'\n",
    "            elif court_folder == 'calcutta_high_court':\n",
    "                court_name = 'Calcutta High Court'\n",
    "            elif court_folder == 'allahabad_high_court':\n",
    "                court_name = 'Allahabad High Court'\n",
    "            elif court_folder == 'madras_high_court':\n",
    "                court_name = 'Madras High Court'\n",
    "            else:\n",
    "                court_name = 'Unknown'\n",
    "            metadata = self.extract_metadata(soup, html_content)\n",
    "            metadata['court'] = court_name         \n",
    "            clean_text = self.clean_text(soup)            \n",
    "            case_data = {\n",
    "                'file_name': html_path.name, 'metadata': metadata,\n",
    "                'text': clean_text, 'text_length': len(clean_text),\n",
    "                'word_count': len(clean_text.split()),'processed_date': datetime.now().isoformat()}\n",
    "            return case_data\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {html_path}: {e}\")\n",
    "            return None    \n",
    "    def process_all(self):\n",
    "        stats = {\n",
    "            'total_processed': 0, 'by_court': {}, 'by_year': {}, 'errors': 0}\n",
    "        for court_dir in self.raw_dir.iterdir():\n",
    "            if not court_dir.is_dir():\n",
    "                continue\n",
    "            court_name = court_dir.name\n",
    "            logger.info(f\"Processing {court_name}\")\n",
    "            html_files = list(court_dir.glob('*.html'))\n",
    "            processed_count = 0\n",
    "            for idx, html_file in enumerate(html_files, 1):\n",
    "                try:\n",
    "                    success = process_single_case(html_file, self.processed_dir)\n",
    "                    if success:\n",
    "                        processed_count += 1\n",
    "                        stats['total_processed'] += 1\n",
    "                        if processed_count % 100 == 0:\n",
    "                            logger.info(f\"Processed {processed_count}/{len(html_files)} cases from {court_name}\")\n",
    "                    else:\n",
    "                        stats['errors'] += 1\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing {html_file}: {e}\")\n",
    "                    stats['errors'] += 1\n",
    "            stats['by_court'][court_name] = processed_count\n",
    "            logger.info(f\"Completed processing {processed_count} cases from {court_name}\")\n",
    "        return stats\n",
    "def process_single_case(html_path: Path, processed_dir: Path) -> bool:\n",
    "    try:\n",
    "        with open(html_path, 'r', encoding='utf-8') as f:\n",
    "            html_content = f.read()\n",
    "        soup = BeautifulSoup(html_content, 'lxml')\n",
    "        court_folder = html_path.parent.name\n",
    "        court_map = {\n",
    "            'supreme_court': 'Supreme Court of India',\n",
    "            'delhi_high_court': 'Delhi High Court',\n",
    "            'bombay_high_court': 'Bombay High Court',\n",
    "            'calcutta_high_court': 'Calcutta High Court',\n",
    "            'allahabad_high_court': 'Allahabad High Court',\n",
    "            'madras_high_court': 'Madras High Court'}\n",
    "        court_name = court_map.get(court_folder, 'Unknown')        \n",
    "        preprocessor = LegalCasePreprocessor()\n",
    "        metadata = preprocessor.extract_metadata(soup, html_content)\n",
    "        metadata['court'] = court_name\n",
    "        clean_text = preprocessor.clean_text(soup)\n",
    "        case_data = {\n",
    "            'file_name': html_path.name, 'metadata': metadata,\n",
    "            'text': clean_text, 'text_length': len(clean_text),\n",
    "            'word_count': len(clean_text.split()),\n",
    "            'processed_date': datetime.now().isoformat()}\n",
    "        json_filename = html_path.stem + '.json'\n",
    "        json_path = processed_dir / court_folder / json_filename\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(case_data, f, indent=2, ensure_ascii=False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        return False\n",
    "class DatasetAnalyzer:\n",
    "    def __init__(self, processed_dir: str = \"dataset_processed\"):\n",
    "        self.processed_dir = Path(processed_dir)    \n",
    "    def generate_stats(self) -> Dict:\n",
    "        stats = {\n",
    "            'total_cases': 0,'by_court': {},\n",
    "            'by_year': {}, 'avg_word_count': 0}\n",
    "        total_words = 0\n",
    "        for court_dir in self.processed_dir.iterdir():\n",
    "            if not court_dir.is_dir():\n",
    "                continue\n",
    "            court_name = court_dir.name\n",
    "            stats['by_court'][court_name] = 0\n",
    "            for json_file in court_dir.glob('*.json'):\n",
    "                try:\n",
    "                    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                        case_data = json.load(f)\n",
    "                    stats['total_cases'] += 1\n",
    "                    stats['by_court'][court_name] += 1                    \n",
    "                    match = re.search(r'_(20\\d{2})_', json_file.name)\n",
    "                    if match:\n",
    "                        year = match.group(1)\n",
    "                        stats['by_year'][year] = stats['by_year'].get(year, 0) + 1\n",
    "                    total_words += case_data.get('word_count', 0)\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error analyzing {json_file}: {e}\")        \n",
    "        if stats['total_cases'] > 0:\n",
    "            stats['avg_word_count'] = total_words // stats['total_cases']        \n",
    "        return stats\n",
    "    def save_stats_report(self, output_file: str = \"dataset_stats.json\"):\n",
    "        stats = self.generate_stats()\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"Statistics saved to {output_file}\")\n",
    "        return stats    \n",
    "    def print_summary(self):\n",
    "        stats = self.generate_stats()\n",
    "        print(\"\\n\")\n",
    "        print(\"Dataset Statistics Summary:\")\n",
    "        print(f\"\\nTotal Cases: {stats['total_cases']}\")\n",
    "        print(f\"\\nCases by Court:\")\n",
    "        for court, count in stats['by_court'].items():\n",
    "            print(f\"  {court}: {count}\")\n",
    "        print(f\"\\nCases by Year:\")\n",
    "        for year, count in stats['by_year'].items():\n",
    "            print(f\"  {year}: {count}\")\n",
    "        print(f\"\\nAverage Word Count: {stats['avg_word_count']}\")\n",
    "        print(\"\\n\")\n",
    "def print_menu():\n",
    "    print(\"\\n\")\n",
    "    print(\"\\n[1] Dataset Extraction\")\n",
    "    print(\"    - Extract legal case documents from the Indian Kanoon website\")\n",
    "    print(\"\\n[2] Preprocessing\")\n",
    "    print(\"    - Process raw HTML files into structured JSON\")\n",
    "    print(\"\\n[3] Generate Statistics\")\n",
    "    print(\"    - Analyze and view dataset statistics\")\n",
    "    print(\"\\n[4] Complete Pipeline\")\n",
    "    print(\"    - Run extraction, preprocessing, and statistics\")\n",
    "    print(\"\\n[0] Exit\")\n",
    "    print(\"\\n\")\n",
    "def extraction_menu():\n",
    "    print(\"\\n\")\n",
    "    print(\"Dataset Extraction Options:\")\n",
    "    print(\"\\n[1] Extract from SPECIFIC COURT and YEAR\")\n",
    "    print(\"[2] Extract from SPECIFIC COURT (all years)\")\n",
    "    print(\"[3] Extract from ALL COURTS (all years)\")\n",
    "    print(\"[0] Back to Main Menu\")\n",
    "    print(\"\\n\")\n",
    "def court_selection_menu():\n",
    "    print(\"\\nAvailable Courts:\")\n",
    "    print(\"[1] Supreme Court of India\")\n",
    "    print(\"[2] Delhi High Court\")\n",
    "    print(\"[3] Bombay High Court\")\n",
    "    print(\"[4] Calcutta High Court\")\n",
    "    print(\"[5] Allahabad High Court\")\n",
    "    print(\"[6] Madras High Court\")\n",
    "def get_court_folder(choice: int) -> Optional[str]:\n",
    "    court_map = {\n",
    "        1: \"supreme_court\",\n",
    "        2: \"delhi_high_court\",\n",
    "        3: \"bombay_high_court\",\n",
    "        4: \"calcutta_high_court\",\n",
    "        5: \"allahabad_high_court\",\n",
    "        6: \"madras_high_court\"}\n",
    "    return court_map.get(choice)\n",
    "def run_extraction(scraper: IndianKanoonScraper):\n",
    "    while True:\n",
    "        extraction_menu()\n",
    "        choice = input(\"\\nEnter your choice: \").strip()\n",
    "        if choice == '0':\n",
    "            break\n",
    "        elif choice == '1':\n",
    "            court_selection_menu()\n",
    "            court_choice = input(\"\\nSelect court (1-6): \").strip()\n",
    "            try:\n",
    "                court_choice = int(court_choice)\n",
    "                court_folder = get_court_folder(court_choice)\n",
    "                if not court_folder:\n",
    "                    print(\"Invalid court selection.\")\n",
    "                    continue\n",
    "                year_input = input(\"Enter year(s) (2000-2024, comma-separated for multiple): \").strip()\n",
    "                years = []\n",
    "                try:\n",
    "                    year_parts = [y.strip() for y in year_input.split(',')]\n",
    "                    for y in year_parts:\n",
    "                        year = int(y)\n",
    "                        if year < 2000 or year > 2024:\n",
    "                            print(f\"Invalid year {year}! Must be between 2000-2024\")\n",
    "                            continue\n",
    "                        if court_folder == \"calcutta_high_court\" and year == 2012:\n",
    "                            print(f\"Skipping {year}: Data not available for Calcutta High Court 2012\")\n",
    "                            continue\n",
    "                        if court_folder == \"allahabad_high_court\" and year == 2009:\n",
    "                            print(f\"Skipping {year}: Data not available for Allahabad High Court 2009\")\n",
    "                            continue\n",
    "                        years.append(year)\n",
    "                    if not years:\n",
    "                        print(\"No valid years to extract.\")\n",
    "                        continue\n",
    "                    max_cases = input(\"Maximum cases per year (default 1000): \").strip()\n",
    "                    max_cases = int(max_cases) if max_cases else 1000\n",
    "                    print(f\"\\nStarting extraction for {court_folder} - Years: {years}\")\n",
    "                    stats = {}\n",
    "                    for year in years:\n",
    "                        config_key = f\"{court_folder}_{year}\"\n",
    "                        print(f\"\\nExtracting {year}...\")\n",
    "                        count = scraper.scrape_court_year(config_key, max_cases)\n",
    "                        stats[year] = count\n",
    "                    print(f\"\\nExtraction complete for {court_folder}!\")\n",
    "                    print(\"Summary:\", stats)\n",
    "                except ValueError:\n",
    "                    print(\"Invalid year format. Please enter valid numbers.\")\n",
    "                    continue\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter valid numbers.\")\n",
    "            except KeyError:\n",
    "                print(\"Configuration not found.\")\n",
    "        elif choice == '2':\n",
    "            court_selection_menu()\n",
    "            court_choice = input(\"\\nSelect court (1-6): \").strip()\n",
    "            try:\n",
    "                court_choice = int(court_choice)\n",
    "                court_folder = get_court_folder(court_choice)\n",
    "                if not court_folder:\n",
    "                    print(\"Invalid court selection.\")\n",
    "                    continue\n",
    "                max_cases = input(\"Maximum cases per year (default 1000): \").strip()\n",
    "                max_cases = int(max_cases) if max_cases else 1000\n",
    "                print(f\"\\nStarting extraction for all years of {court_folder}...\")\n",
    "                stats = {}\n",
    "                for year in range(2000, 2025):\n",
    "                    if court_folder == \"calcutta_high_court\" and year == 2012:\n",
    "                        continue\n",
    "                    if court_folder == \"allahabad_high_court\" and year == 2009:\n",
    "                        continue\n",
    "                    config_key = f\"{court_folder}_{year}\"\n",
    "                    if config_key in scraper.search_configs:\n",
    "                        print(f\"\\nExtracting {year}...\")\n",
    "                        count = scraper.scrape_court_year(config_key, max_cases)\n",
    "                        stats[year] = count\n",
    "                print(f\"\\nExtraction completed for {court_folder}.\")\n",
    "                print(\"Summary:\", stats)\n",
    "            except ValueError:\n",
    "                print(\"Invalid input. Please enter valid numbers.\")\n",
    "        elif choice == '3':\n",
    "            max_cases = input(\"Maximum cases per year per court (default 1000): \").strip()\n",
    "            max_cases = int(max_cases) if max_cases else 1000\n",
    "            print(\"\\nStarting extraction for ALL courts and years...\")\n",
    "            confirm = input(\"Continue? (yes/no): \").strip().lower()\n",
    "            if confirm == 'yes':\n",
    "                stats = {}\n",
    "                for config_key in scraper.search_configs:\n",
    "                    print(f\"\\nExtracting {config_key}...\")\n",
    "                    count = scraper.scrape_court_year(config_key, max_cases)\n",
    "                    stats[config_key] = count\n",
    "                print(\"\\nExtraction complete for all courts.\")\n",
    "                print(\"Summary:\", stats)\n",
    "            else:\n",
    "                print(\"Extraction cancelled.\")\n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "def run_preprocessing():\n",
    "    print(\"\\n\")\n",
    "    print(\"Preprocessing Raw Dataset\")\n",
    "    print(\"\\n\")\n",
    "    preprocessor = LegalCasePreprocessor()\n",
    "    if not preprocessor.raw_dir.exists():\n",
    "        print(\"\\nError: 'dataset_raw' directory not found.\")\n",
    "        print(\"Please run extraction first.\")\n",
    "        return\n",
    "    raw_count = sum(1 for court_dir in preprocessor.raw_dir.iterdir() \n",
    "                    if court_dir.is_dir() \n",
    "                    for _ in court_dir.glob('*.html'))\n",
    "    if raw_count == 0:\n",
    "        print(\"\\nNo raw HTML files found in 'dataset_raw' directory.\")\n",
    "        print(\"Please run extraction first.\")\n",
    "        return\n",
    "    print(f\"\\nFound {raw_count} raw HTML files to process.\")\n",
    "    confirm = input(\"\\nStart preprocessing? (yes/no): \").strip().lower()\n",
    "    if confirm == 'yes':\n",
    "        print(\"\\nStarting preprocessing...\")\n",
    "        stats = preprocessor.process_all()\n",
    "        print(\"\\n\")\n",
    "        print(\"Preprocessing Completed.\")\n",
    "        print(f\"\\nTotal Processed: {stats['total_processed']}\")\n",
    "        print(f\"Errors: {stats['errors']}\")\n",
    "        print(f\"\\nProcessed by Court:\")\n",
    "        for court, count in stats['by_court'].items():\n",
    "            print(f\"  {court}: {count}\")\n",
    "        print(\"\\n\")\n",
    "    else:\n",
    "        print(\"Preprocessing cancelled.\")\n",
    "def run_statistics():\n",
    "    print(\"\\n\")\n",
    "    print(\"Dataset Statistics\")\n",
    "    print(\"\\n\")\n",
    "    analyzer = DatasetAnalyzer()\n",
    "    if not analyzer.processed_dir.exists():\n",
    "        print(\"\\nError: 'dataset_processed' directory not found.\")\n",
    "        print(\"Please run preprocessing first.\")\n",
    "        return    \n",
    "    processed_count = sum(1 for court_dir in analyzer.processed_dir.iterdir() \n",
    "                          if court_dir.is_dir() \n",
    "                          for _ in court_dir.glob('*.json'))\n",
    "    if processed_count == 0:\n",
    "        print(\"\\nNo processed JSON files found in 'dataset_processed' directory.\")\n",
    "        print(\"Please run preprocessing first.\")\n",
    "        return\n",
    "    print(f\"\\nFound {processed_count} processed files.\")\n",
    "    print(\"\\nGenerating statistics...\")\n",
    "    analyzer.save_stats_report()\n",
    "    analyzer.print_summary()\n",
    "def main():\n",
    "    scraper = IndianKanoonScraper()\n",
    "    while True:\n",
    "        print_menu()\n",
    "        choice = input(\"\\nEnter your choice: \").strip()\n",
    "        if choice == '0':\n",
    "            print(\"\\nThe system ends.\")\n",
    "            break\n",
    "        elif choice == '1':\n",
    "            run_extraction(scraper)\n",
    "        elif choice == '2':\n",
    "            run_preprocessing()\n",
    "        elif choice == '3':\n",
    "            run_statistics()\n",
    "        elif choice == '4':\n",
    "            print(\"\\n\")\n",
    "            print(\"Complete Pipeline -\")\n",
    "            print(\"\\nThis will run:\")\n",
    "            print(\"1. Dataset Extraction\")\n",
    "            print(\"2. Preprocessing\")\n",
    "            print(\"3. Statistics Generation\")\n",
    "            confirm = input(\"\\nContinue with complete pipeline? (yes/no): \").strip().lower()            \n",
    "            if confirm == 'yes':\n",
    "                max_cases = input(\"\\nMaximum cases per year per court (default 1000): \").strip()\n",
    "                max_cases = int(max_cases) if max_cases else 1000\n",
    "                print(\"\\n[STEP 1/3] Starting extraction for ALL courts and years...\")\n",
    "                stats = {}\n",
    "                for config_key in scraper.search_configs:\n",
    "                    print(f\"\\nExtracting {config_key}...\")\n",
    "                    count = scraper.scrape_court_year(config_key, max_cases)\n",
    "                    stats[config_key] = count\n",
    "                print(\"\\nExtraction completed.\")\n",
    "                print(\"Summary:\", stats)                \n",
    "                print(\"\\n[STEP 2/3] Starting preprocessing...\")\n",
    "                preprocessor = LegalCasePreprocessor()\n",
    "                processing_stats = preprocessor.process_all()\n",
    "                print(f\"\\nPreprocessing completed. Processed: {processing_stats['total_processed']}, Errors: {processing_stats['errors']}\")                \n",
    "                print(\"\\n[STEP 3/3] Generating statistics...\")\n",
    "                analyzer = DatasetAnalyzer()\n",
    "                analyzer.save_stats_report()\n",
    "                analyzer.print_summary()\n",
    "                print(\"\\nComplete Pipeline Finished Successfully.\")\n",
    "            else:\n",
    "                print(\"Pipeline cancelled.\")\n",
    "        else:\n",
    "            print(\"Invalid choice. Please try again.\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
