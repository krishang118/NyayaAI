{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 09:36:39,478 - WARNING - Spacy model not found.\n",
      "2025-11-06 09:36:39,478 - INFO - Loading ontology from IndiLegalOnt.owl...\n",
      "* Owlready2 * WARNING: ObjectProperty http://lmss.sali.org/R7PEVL4EDe99UD7dVx58cFp belongs to more than one entity types: [owl.AnnotationProperty, owl.ObjectProperty]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: ObjectProperty http://lmss.sali.org/R9e8PXI2IEusTTC9xKXN7hA belongs to more than one entity types: [owl.AnnotationProperty, owl.ObjectProperty]; I'm trying to fix it...\n",
      "* Owlready2 * WARNING: ObjectProperty http://lmss.sali.org/RTVHIHUY48KidG92GXtTlH belongs to more than one entity types: [owl.AnnotationProperty, owl.ObjectProperty]; I'm trying to fix it...\n",
      "2025-11-06 09:36:40,194 - INFO - Ontology loaded: http://lmss.sali.org/\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set, Tuple, Optional\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "from owlready2 import get_ontology\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import random\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "try:\n",
    "    import spacy\n",
    "    SPACY_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SPACY_AVAILABLE = False\n",
    "    logger.warning(\"spaCy not available. NLP entity extraction will be limited.\")\n",
    "logging.getLogger('matplotlib.legend').setLevel(logging.ERROR)\n",
    "class LegalKnowledgeGraphPipeline:    \n",
    "    def __init__(self, owl_path: str, processed_dir: str = \"dataset_processed\", \n",
    "                 rules_dir: str = \"official_documents\", cache_dir: str = \"entity_cache\"):\n",
    "        self.processed_dir = Path(processed_dir)\n",
    "        self.rules_dir = Path(rules_dir)\n",
    "        self.owl_path = owl_path        \n",
    "        self.onto = None\n",
    "        self.cases_data = {}\n",
    "        self.entity_mappings = {}\n",
    "        self.global_knowledge_graph = nx.DiGraph()\n",
    "        self.legal_doctrines = self._initialize_legal_doctrines()\n",
    "        self.court_hierarchy = {\n",
    "            'supreme_court': 1, 'allahabad_high_court': 2,\n",
    "            'bombay_high_court': 2, 'calcutta_high_court': 2,\n",
    "            'delhi_high_court': 2, 'madras_high_court': 2}        \n",
    "        self.stats = {\n",
    "            'total_cases': 0, 'by_court': defaultdict(int),\n",
    "            'by_year': defaultdict(int)}\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        self.entity_cache_enabled = True        \n",
    "        if SPACY_AVAILABLE:\n",
    "            try:\n",
    "                self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "            except:\n",
    "                logger.warning(\"Spacy model not found.\")\n",
    "                self.nlp = None\n",
    "        else:\n",
    "            self.nlp = None\n",
    "        self.load_ontology()\n",
    "    def _initialize_legal_doctrines(self) -> Dict[str, List[str]]:\n",
    "        return {\n",
    "            'constitutional': ['fundamental rights', 'directive principles', 'judicial review'],\n",
    "            'procedural': ['natural justice', 'due process', 'res judicata'],\n",
    "            'criminal': ['presumption of innocence', 'burden of proof', 'reasonable doubt'],\n",
    "            'civil': ['promissory estoppel', 'unjust enrichment', 'specific performance'],\n",
    "            'administrative': ['legitimate expectation', 'proportionality', 'ultra vires']}\n",
    "    def load_ontology(self):\n",
    "        logger.info(f\"Loading ontology from {self.owl_path}...\")\n",
    "        try:\n",
    "            self.onto = get_ontology(self.owl_path).load()\n",
    "            logger.info(f\"Ontology loaded: {self.onto.base_iri}\")\n",
    "            self._index_ontology_entities()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading ontology: {e}\")\n",
    "            raise\n",
    "    def _index_ontology_entities(self):\n",
    "        self.onto_index = {'classes': {}, 'individuals': {}}\n",
    "        for cls in self.onto.classes():\n",
    "            labels = [str(l).lower() for l in cls.label] if cls.label else []\n",
    "            self.onto_index['classes'][cls.name.lower()] = cls\n",
    "            for label in labels:\n",
    "                self.onto_index['classes'][label] = cls\n",
    "        for ind in self.onto.individuals():\n",
    "            labels = [str(l).lower() for l in ind.label] if ind.label else []\n",
    "            self.onto_index['individuals'][ind.name.lower()] = ind\n",
    "            for label in labels:\n",
    "                self.onto_index['individuals'][label] = ind\t    \n",
    "    def load_official_documents(self):\n",
    "        logger.info(\"Loading official legal documents from PDFs...\")\n",
    "        self.official_docs = {}\n",
    "        doc_files = {\n",
    "            'ipc': 'Indian Penal Code.pdf', 'crpc': 'Code of Criminal Procedure.pdf',\n",
    "            'constitution': 'Constitution of India.pdf', 'evidence': 'Indian Evidence Act.pdf'}\n",
    "        for doc_type, filename in doc_files.items():\n",
    "            filepath = self.rules_dir / filename\n",
    "            if filepath.exists():\n",
    "                try:\n",
    "                    logger.info(f\"Loading {filename}...\")\n",
    "                    extracted_data = self._extract_provisions_from_pdf(filepath, doc_type)\n",
    "                    if extracted_data:\n",
    "                        self.official_docs[doc_type] = extracted_data\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Could not load {filename}: {e}\")\n",
    "            else:\n",
    "                logger.warning(f\"File not found: {filepath}\")\n",
    "        if self.official_docs:\n",
    "            self._create_provision_index()\n",
    "        else:\n",
    "            logger.warning(\"No official documents loaded.\")\n",
    "    def _extract_provisions_from_pdf(self, pdf_path: Path, doc_type: str) -> Dict:\n",
    "        try:\n",
    "            import PyPDF2            \n",
    "            extracted_data = {'provisions': [], 'full_text': ''}\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                pdf_reader = PyPDF2.PdfReader(file)\n",
    "                full_text = \"\"\n",
    "                logger.info(f\"  Reading {len(pdf_reader.pages)} pages...\")\n",
    "                for page in pdf_reader.pages:\n",
    "                    text = page.extract_text()\n",
    "                    if text:\n",
    "                        full_text += text + \"\\n\"\n",
    "                extracted_data['full_text'] = full_text\n",
    "                if doc_type == 'constitution':\n",
    "                    provisions = self._extract_articles_from_text(full_text)\n",
    "                else:\n",
    "                    provisions = self._extract_sections_from_text(full_text)\n",
    "                extracted_data['provisions'] = provisions\n",
    "                return extracted_data\n",
    "        except ImportError:\n",
    "            logger.error(\"PyPDF2 not installed.\")\n",
    "            return {}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting from PDF: {e}\")\n",
    "            return {}\n",
    "    def _extract_sections_from_text(self, text: str) -> List[Dict]:\n",
    "        sections = []\n",
    "        section_pattern = r'(?:Section\\s+)?(\\d+[A-Z]*)\\.\\s*([^\\n]{10,200}?)'\n",
    "        matches = re.finditer(section_pattern, text, re.MULTILINE)\n",
    "        for match in matches:\n",
    "            section_num = match.group(1).strip()\n",
    "            section_text = match.group(2).strip()\n",
    "            section_text = re.sub(r'\\s+', ' ', section_text)\n",
    "            sections.append({\n",
    "                'number': section_num, 'text': section_text[:500],\n",
    "                'title': section_text.split('.')[0][:100] if '.' in section_text else section_text[:100]})\n",
    "        return sections[:500]\n",
    "    def _extract_articles_from_text(self, text: str) -> List[Dict]:\n",
    "        articles = []\n",
    "        article_pattern = r'(?:Article\\s+)?(\\d+[A-Z]*)\\.\\s*([^\\n]{10,200}?)'\n",
    "        matches = re.finditer(article_pattern, text, re.MULTILINE)\n",
    "        for match in matches:\n",
    "            article_num = match.group(1).strip()\n",
    "            article_text = match.group(2).strip()\n",
    "            article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "            articles.append({\n",
    "                'number': article_num, 'text': article_text[:500],\n",
    "                'title': article_text.split('.')[0][:100] if '.' in article_text else article_text[:100]})\n",
    "        return articles[:500]\n",
    "    def _create_provision_index(self):\n",
    "        self.provision_index = defaultdict(dict)\n",
    "        self.provision_to_act_map = {}\n",
    "        doc_name_map = {\n",
    "            'ipc': 'Indian Penal Code', 'crpc': 'Code of Criminal Procedure',\n",
    "            'constitution': 'Constitution of India', 'evidence': 'Indian Evidence Act'}\n",
    "        for doc_type, doc_data in self.official_docs.items():\n",
    "            act_name = doc_name_map.get(doc_type, doc_type)\n",
    "            if 'provisions' in doc_data:\n",
    "                for provision in doc_data['provisions']:\n",
    "                    prov_num = provision.get('number', '')\n",
    "                    self.provision_index[doc_type][prov_num] = {\n",
    "                        'text': provision.get('text', ''),\n",
    "                        'title': provision.get('title', ''),\n",
    "                        'act': act_name}\n",
    "                    if doc_type == 'constitution':\n",
    "                        self.provision_to_act_map[f\"Article {prov_num}\"] = act_name\n",
    "                    else:\n",
    "                        self.provision_to_act_map[f\"Section {prov_num}\"] = act_name\n",
    "        total = sum(len(v) for v in self.provision_index.values())\n",
    "        logger.info(f\"Indexed {total} provisions\")\n",
    "        for doc_type, provisions in self.provision_index.items():\n",
    "            logger.info(f\"  {doc_name_map.get(doc_type)}: {len(provisions)} provisions\")\n",
    "    def load_all_cases(self):\n",
    "        logger.info(\"Loading all case files...\")\n",
    "        courts = [\n",
    "            'supreme_court', 'allahabad_high_court', 'bombay_high_court',\n",
    "            'calcutta_high_court', 'delhi_high_court', 'madras_high_court']\n",
    "        for court in courts:\n",
    "            court_dir = self.processed_dir / court\n",
    "            if not court_dir.exists():\n",
    "                logger.warning(f\"Directory not found: {court_dir}\")\n",
    "                continue\n",
    "            self.cases_data[court] = []\n",
    "            json_files = list(court_dir.glob(\"*.json\"))\n",
    "            logger.info(f\"Loading {len(json_files)} cases from {court}...\")\n",
    "            for json_file in tqdm(json_files, desc=f\"Loading {court}\"):\n",
    "                try:\n",
    "                    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                        case_data = json.load(f)\n",
    "                        case_data['court_name'] = court\n",
    "                        self.cases_data[court].append(case_data)\n",
    "                        self.stats['by_court'][court] += 1\n",
    "                        year = self._extract_year(case_data)\n",
    "                        if year:\n",
    "                            self.stats['by_year'][year] += 1\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error loading {json_file}: {e}\")\n",
    "            logger.info(f\"Loaded {len(self.cases_data[court])} cases from {court}\")\n",
    "        self.stats['total_cases'] = sum(self.stats['by_court'].values())\n",
    "        logger.info(f\"Total cases loaded: {self.stats['total_cases']}\")    \n",
    "    def _extract_year(self, case: Dict) -> Optional[str]:\n",
    "        if 'metadata' in case and 'date' in case['metadata']:\n",
    "            date_str = case['metadata']['date']\n",
    "            try:\n",
    "                date_obj = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                return str(date_obj.year)\n",
    "            except:\n",
    "                pass\n",
    "        if 'file_name' in case:\n",
    "            match = re.search(r'_(19\\d{2}|20\\d{2})_', case['file_name'])\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        return None\n",
    "    def entity_extraction(self, case: Dict) -> Dict:\n",
    "        case_id = case.get('file_name', '')\n",
    "        if self.entity_cache_enabled and case_id:\n",
    "            cache_file = self.cache_dir / f\"{case_id.replace('/', '_')}.json\"\n",
    "            if cache_file.exists():\n",
    "                try:\n",
    "                    with open(cache_file, 'r', encoding='utf-8') as f:\n",
    "                        return json.load(f)\n",
    "                except:\n",
    "                    pass        \n",
    "        entities = self._extract_entities_internal(case)\n",
    "        if self.entity_cache_enabled and case_id:\n",
    "            cache_file = self.cache_dir / f\"{case_id.replace('/', '_')}.json\"\n",
    "            try:\n",
    "                with open(cache_file, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(entities, f)\n",
    "            except:\n",
    "                pass\n",
    "        return entities\n",
    "    def _extract_entities_internal(self, case: Dict) -> Dict:\n",
    "        entities = {\n",
    "            'court': None, 'parties': {'petitioner': [], 'respondent': []}, 'judges': [],\n",
    "            'provisions': {'sections': [], 'articles': [], 'rules': [], 'orders': []},\n",
    "            'citations': [], 'acts': [], 'legal_concepts': [], 'case_types': [],\n",
    "            'doctrines': [], 'outcomes': []}        \n",
    "        metadata = case.get('metadata', {})\n",
    "        entities['court'] = case.get('court_name', metadata.get('court', ''))\n",
    "        entities['date'] = metadata.get('date', '')        \n",
    "        if metadata.get('petitioner'):\n",
    "            entities['parties']['petitioner'] = self._normalize_party_name(metadata['petitioner'])\n",
    "        if metadata.get('respondent'):\n",
    "            entities['parties']['respondent'] = self._normalize_party_name(metadata['respondent'])\n",
    "        entities['citations'] = metadata.get('citations', [])[:15]\n",
    "        text = case.get('text', '')\n",
    "        if not text:\n",
    "            return entities\n",
    "        text_sample = text[:45000]        \n",
    "        entities['provisions'] = self._extract_provisions(text_sample)\n",
    "        entities['acts'] = self._extract_acts(text_sample)\n",
    "        entities['legal_concepts'] = self._extract_legal_concepts(text_sample)\n",
    "        entities['doctrines'] = self._extract_doctrines(text_sample)\n",
    "        entities['case_types'] = self._classify_case_type(text_sample, metadata)\n",
    "        entities['judges'] = self._extract_judges(text_sample)\n",
    "        entities['outcomes'] = self._extract_outcomes(text_sample)        \n",
    "        entities['ontology_mappings'] = self._map_to_ontology_entities(entities)        \n",
    "        entities = self.enrich_entities_with_rulebooks(entities)\n",
    "        return entities\n",
    "    def _normalize_party_name(self, party_name: str) -> List[str]:\n",
    "        if not party_name:\n",
    "            return []\n",
    "        party_name = re.sub(r'\\s+(Ltd\\.|Limited|Pvt\\.|Private|Co\\.)\\.?$', \n",
    "                           '', party_name, flags=re.IGNORECASE)\n",
    "        parties = re.split(r'\\s+(?:vs?\\.?|versus|and others?)\\s+', \n",
    "                          party_name, flags=re.IGNORECASE)\n",
    "        return [p.strip() for p in parties if p.strip()][:3]\n",
    "    def _extract_provisions(self, text: str) -> Dict[str, List[str]]:\n",
    "        provisions = {'sections': [], 'articles': [], 'rules': [], 'orders': []}        \n",
    "        patterns = {\n",
    "            'sections': [r\"Section\\s+(\\d+[A-Z]*(?:\\s*\\([a-z0-9]+\\))*)\"],\n",
    "            'articles': [r\"Article\\s+(\\d+[A-Z]*(?:\\s*\\([a-z0-9]+\\))*)\"],\n",
    "            'rules': [r\"Rule\\s+(\\d+[A-Z]*)\"], 'orders': [r\"Order\\s+([IVX]+)\"]}\n",
    "        for prov_type, pattern_list in patterns.items():\n",
    "            for pattern in pattern_list:\n",
    "                matches = re.findall(pattern, text, re.IGNORECASE)\n",
    "                provisions[prov_type].extend(matches)\n",
    "        for prov_type in provisions:\n",
    "            provisions[prov_type] = list(set([p.strip() for p in provisions[prov_type] if p.strip()]))[:30]\n",
    "        return provisions\n",
    "    def _extract_acts(self, text: str) -> List[Dict[str, str]]:\n",
    "        act_pattern = r\"([A-Z][A-Za-z\\s&,\\(\\)]+(?:Act|Code)(?:\\s*,?\\s*(\\d{4}))?)\"\n",
    "        matches = re.findall(act_pattern, text)\n",
    "        acts = []\n",
    "        seen = set()\n",
    "        for act_name, year in matches:\n",
    "            act_name = act_name.strip()\n",
    "            if 10 < len(act_name) < 150 and act_name not in seen:\n",
    "                acts.append({'name': act_name, 'year': year if year else None})\n",
    "                seen.add(act_name)\n",
    "        return acts[:25]    \n",
    "    def _extract_legal_concepts(self, text: str) -> List[str]:\n",
    "        concept_pattern = (\n",
    "            r\"\\b(jurisdiction|appeal|writ|mandamus|certiorari|\"\n",
    "            r\"natural justice|due process|fundamental right|\"\n",
    "            r\"res judicata|mens rea|actus reus|burden of proof|\"\n",
    "            r\"negligence|damages|injunction)\\b\")\n",
    "        concepts = re.findall(concept_pattern, text, re.IGNORECASE)\n",
    "        return list(set([c.title() for c in concepts]))[:40]    \n",
    "    def _extract_doctrines(self, text: str) -> List[str]:\n",
    "        doctrines = []\n",
    "        text_lower = text.lower()\n",
    "        for doctrine_type, doctrine_list in self.legal_doctrines.items():\n",
    "            for doctrine in doctrine_list:\n",
    "                if doctrine.lower() in text_lower:\n",
    "                    doctrines.append(doctrine)        \n",
    "        return list(set(doctrines))    \n",
    "    def _classify_case_type(self, text: str, metadata: Dict) -> List[str]:\n",
    "        case_types = []\n",
    "        text_lower = text.lower()        \n",
    "        type_patterns = {\n",
    "            'Criminal': [\n",
    "                ('criminal appeal', 2), ('conviction', 1), ('ipc', 1), \n",
    "                ('penal code', 1), ('accused', 2)],\n",
    "            'Civil': [\n",
    "                ('civil appeal', 2), ('contract', 1), ('property dispute', 2),\n",
    "                ('damages', 1), ('plaintiff', 2)],\n",
    "            'Constitutional': [\n",
    "                ('writ petition', 2), ('article 32', 2), ('article 226', 2),\n",
    "                ('fundamental right', 2), ('constitutional validity', 2)],\n",
    "            'Tax': [\n",
    "                ('income tax', 2), ('sales tax', 1), ('customs', 1),\n",
    "                ('tax appeal', 2), ('assessment', 1)],\n",
    "            'Service': [\n",
    "                ('service matter', 2), ('employment', 1), ('termination', 1),\n",
    "                ('pension', 1)]}\n",
    "        for case_type, patterns in type_patterns.items():\n",
    "            score = 0\n",
    "            for keyword, weight in patterns:\n",
    "                if keyword in text_lower:\n",
    "                    score += weight            \n",
    "            threshold = 2 if case_type == 'Constitutional' else 1\n",
    "            if score >= threshold:\n",
    "                case_types.append(case_type)\n",
    "        return list(set(case_types))[:5] if case_types else ['General']    \n",
    "    def _extract_judges(self, text: str) -> List[str]:\n",
    "        judge_patterns = [r\"(?:Hon'ble\\s+)?Justice\\s+([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\"]        \n",
    "        judges = []\n",
    "        for pattern in judge_patterns:\n",
    "            matches = re.findall(pattern, text[:5000])\n",
    "            judges.extend(matches)\n",
    "        return list(set(judges))[:10]    \n",
    "    def _extract_outcomes(self, text: str) -> List[str]:\n",
    "        outcomes = []\n",
    "        text_lower = text.lower()        \n",
    "        outcome_keywords = {\n",
    "            'allowed': ['appeal allowed', 'petition allowed'],\n",
    "            'dismissed': ['appeal dismissed', 'petition dismissed'],\n",
    "            'quashed': ['order quashed', 'judgment quashed'],\n",
    "            'upheld': ['upheld', 'affirmed']}\n",
    "        for outcome, keywords in outcome_keywords.items():\n",
    "            if any(keyword in text_lower for keyword in keywords):\n",
    "                outcomes.append(outcome)\n",
    "        return outcomes\n",
    "    def _map_to_ontology_entities(self, entities: Dict) -> Dict[str, List[str]]:\n",
    "        mappings = {'classes': [], 'individuals': []}\n",
    "        for prov_list in entities['provisions'].values():\n",
    "            for prov in prov_list:\n",
    "                prov_lower = prov.lower()\n",
    "                if prov_lower in self.onto_index['individuals']:\n",
    "                    mappings['individuals'].append(prov)\n",
    "        for concept in entities.get('legal_concepts', []):\n",
    "            concept_lower = concept.lower()\n",
    "            if concept_lower in self.onto_index['classes']:\n",
    "                mappings['classes'].append(concept)\n",
    "        return mappings\n",
    "    def enrich_entities_with_rulebooks(self, entities: Dict) -> Dict:\n",
    "        enriched = entities.copy()\n",
    "        enriched['validated_provisions'] = []\n",
    "        enriched['provision_details'] = {}\n",
    "        enriched['detected_rulebooks'] = set()        \n",
    "        for prov_type, prov_list in entities['provisions'].items():\n",
    "            for provision in prov_list:\n",
    "                matched_doc, matched_details = self._match_provision_to_rulebook(provision, prov_type)\n",
    "                if matched_doc:\n",
    "                    enriched['validated_provisions'].append(provision)\n",
    "                    enriched['provision_details'][provision] = {\n",
    "                        'rulebook': matched_doc,\n",
    "                        'full_text': matched_details.get('text', ''),\n",
    "                        'title': matched_details.get('title', ''),\n",
    "                        'act': matched_details.get('act', '')}\n",
    "                    enriched['detected_rulebooks'].add(matched_doc)\n",
    "        enriched['act_to_rulebook_map'] = {}\n",
    "        for act in entities.get('acts', []):\n",
    "            act_name = act['name'] if isinstance(act, dict) else act\n",
    "            rulebook = self._map_act_to_rulebook(act_name)\n",
    "            if rulebook:\n",
    "                enriched['act_to_rulebook_map'][act_name] = rulebook\n",
    "                enriched['detected_rulebooks'].add(rulebook)\n",
    "        enriched['detected_rulebooks'] = list(enriched['detected_rulebooks'])\n",
    "        return enriched\n",
    "    def _match_provision_to_rulebook(self, provision: str, prov_type: str) -> Tuple[Optional[str], Dict]:\n",
    "        match = re.search(r'(\\d+[A-Z]*)', provision)\n",
    "        if not match:\n",
    "            return None, {}\n",
    "        prov_num = match.group(1)\n",
    "        if prov_type == 'sections':\n",
    "            if prov_num in self.provision_index.get('ipc', {}):\n",
    "                return 'Indian Penal Code', self.provision_index['ipc'][prov_num]\n",
    "            if prov_num in self.provision_index.get('crpc', {}):\n",
    "                return 'Code of Criminal Procedure', self.provision_index['crpc'][prov_num]\n",
    "            if prov_num in self.provision_index.get('evidence', {}):\n",
    "                return 'Indian Evidence Act', self.provision_index['evidence'][prov_num]\n",
    "        elif prov_type == 'articles':\n",
    "            if prov_num in self.provision_index.get('constitution', {}):\n",
    "                return 'Constitution of India', self.provision_index['constitution'][prov_num]\n",
    "        return None, {}    \n",
    "    def _map_act_to_rulebook(self, act_name: str) -> Optional[str]:\n",
    "        act_lower = act_name.lower()\n",
    "        if any(k in act_lower for k in ['penal code', 'ipc']):\n",
    "            return 'Indian Penal Code'\n",
    "        elif any(k in act_lower for k in ['criminal procedure', 'crpc']):\n",
    "            return 'Code of Criminal Procedure'\n",
    "        elif 'constitution' in act_lower:\n",
    "            return 'Constitution of India'\n",
    "        elif 'evidence' in act_lower:\n",
    "            return 'Indian Evidence Act'\n",
    "        return None\n",
    "    def create_court_knowledge_graph(self, court_name: str, num_cases: int = 10,\n",
    "                                    random_sample: bool = True) -> nx.DiGraph:\n",
    "        logger.info(f\"Creating knowledge graph for {court_name}...\")        \n",
    "        G = nx.DiGraph()\n",
    "        court_cases = self.cases_data.get(court_name, [])\n",
    "        if not court_cases:\n",
    "            logger.warning(f\"No cases found for {court_name}\")\n",
    "            return G        \n",
    "        if random_sample and len(court_cases) > num_cases:\n",
    "            sampled_cases = random.sample(court_cases, num_cases)\n",
    "        else:\n",
    "            sampled_cases = court_cases[:num_cases]        \n",
    "        court_node_id = f\"Court_{court_name}\"\n",
    "        G.add_node(court_node_id, \n",
    "                  node_type='court', label=court_name.replace('_', ' ').title(),\n",
    "                  hierarchy_level=self.court_hierarchy.get(court_name, 2),\n",
    "                  size=7000)        \n",
    "        rulebook_nodes = {}\n",
    "        for rulebook in ['Indian Penal Code', 'Code of Criminal Procedure', \n",
    "                        'Constitution of India', 'Indian Evidence Act']:\n",
    "            rb_node_id = f\"Rulebook_{rulebook.replace(' ', '_')}\"\n",
    "            G.add_node(rb_node_id, node_type='rulebook', label=rulebook, size=5000)\n",
    "            rulebook_nodes[rulebook] = rb_node_id        \n",
    "        global_acts = {}\n",
    "        global_provisions = {}        \n",
    "        for idx, case in enumerate(sampled_cases):\n",
    "            case_id = case.get('file_name', f\"{court_name}_{idx}\")\n",
    "            entities = self.entity_extraction(case)\n",
    "            self.entity_mappings[case_id] = entities            \n",
    "            case_node_id = f\"Case_{court_name}_{idx+1}\"\n",
    "            case_types = entities.get('case_types', ['General'])\n",
    "            year = self._extract_year(case)\n",
    "            G.add_node(case_node_id, node_type='case',\n",
    "                      label=f\"Case {idx+1}\", case_types=case_types,\n",
    "                      year=year, court=court_name, size=3000,\n",
    "                      validated_provisions=len(entities.get('validated_provisions', [])))\n",
    "            G.add_edge(court_node_id, case_node_id, relation='adjudicated', weight=1.0)\n",
    "            for rulebook in entities.get('detected_rulebooks', []):\n",
    "                if rulebook in rulebook_nodes:\n",
    "                    G.add_edge(case_node_id, rulebook_nodes[rulebook],\n",
    "                              relation='references_rulebook', weight=1.0)\n",
    "            for party_type in ['petitioner', 'respondent']:\n",
    "                parties = entities['parties'].get(party_type, [])\n",
    "                for party in parties[:2]:\n",
    "                    party_node_id = f\"Party_{party[:30]}\"\n",
    "                    if party_node_id not in G:\n",
    "                        G.add_node(party_node_id, node_type='party', \n",
    "                                  label=party[:30], size=2200)\n",
    "                    G.add_edge(case_node_id, party_node_id, relation=party_type, weight=1.0)\n",
    "            all_provisions = []\n",
    "            for prov_type, prov_list in entities['provisions'].items():\n",
    "                all_provisions.extend([(prov_type, p) for p in prov_list[:5]])\n",
    "            for prov_type, provision in all_provisions[:8]:\n",
    "                prov_details = entities.get('provision_details', {}).get(provision, {})\n",
    "                parent_act = prov_details.get('act', 'Unknown')\n",
    "                prov_key = f\"{parent_act}_{prov_type}_{provision[:25]}\"\n",
    "                is_validated = provision in entities.get('validated_provisions', [])\n",
    "                parent_rulebook = prov_details.get('rulebook', '')                \n",
    "                if prov_key not in global_provisions:\n",
    "                    prov_node_id = f\"Provision_{len(global_provisions)}\"\n",
    "                    global_provisions[prov_key] = prov_node_id\n",
    "                    G.add_node(prov_node_id,\n",
    "                              node_type='provision',\n",
    "                              label=provision[:25],\n",
    "                              size=2000 if is_validated else 1800,\n",
    "                              validated=is_validated,\n",
    "                              rulebook=parent_rulebook,\n",
    "                              citation_count=1)\n",
    "                else:\n",
    "                    prov_node_id = global_provisions[prov_key]\n",
    "                    G.nodes[prov_node_id]['citation_count'] = G.nodes[prov_node_id].get('citation_count', 0) + 1\n",
    "                G.add_edge(case_node_id, prov_node_id, relation='cites_provision',\n",
    "                          weight=1.5 if is_validated else 1.0)\n",
    "                if is_validated and parent_rulebook and parent_rulebook in rulebook_nodes:\n",
    "                    if not G.has_edge(prov_node_id, rulebook_nodes[parent_rulebook]):\n",
    "                        G.add_edge(prov_node_id, rulebook_nodes[parent_rulebook],\n",
    "                                  relation='defined_in', weight=2.0)            \n",
    "            for act in entities.get('acts', [])[:5]:\n",
    "                act_name = act['name'] if isinstance(act, dict) else act\n",
    "                act_key = act_name[:50]\n",
    "                mapped_rulebook = entities.get('act_to_rulebook_map', {}).get(act_name, '')                \n",
    "                if act_key not in global_acts:\n",
    "                    act_node_id = f\"Act_{len(global_acts)}\"\n",
    "                    global_acts[act_key] = act_node_id\n",
    "                    G.add_node(act_node_id, node_type='act', label=act_name[:50],\n",
    "                              size=2600 if mapped_rulebook else 2400,\n",
    "                              citation_count=1)\n",
    "                else:\n",
    "                    act_node_id = global_acts[act_key]\n",
    "                    G.nodes[act_node_id]['citation_count'] = G.nodes[act_node_id].get('citation_count', 0) + 1\n",
    "                G.add_edge(case_node_id, act_node_id, relation='governed_by',\n",
    "                          weight=1.5 if mapped_rulebook else 1.0)\n",
    "                if mapped_rulebook and mapped_rulebook in rulebook_nodes:\n",
    "                    if not G.has_edge(act_node_id, rulebook_nodes[mapped_rulebook]):\n",
    "                        G.add_edge(act_node_id, rulebook_nodes[mapped_rulebook],\n",
    "                                  relation='codified_in', weight=2.0)\n",
    "            for concept in entities.get('legal_concepts', [])[:6]:\n",
    "                concept_node_id = f\"Concept_{concept[:30]}\"\n",
    "                if concept_node_id not in G:\n",
    "                    G.add_node(concept_node_id, node_type='concept', \n",
    "                              label=concept[:30], size=1600)\n",
    "                G.add_edge(case_node_id, concept_node_id, \n",
    "                          relation='involves_concept', weight=1.0)\n",
    "            for doctrine in entities.get('doctrines', [])[:4]:\n",
    "                doctrine_node_id = f\"Doctrine_{doctrine[:30]}\"\n",
    "                if doctrine_node_id not in G:\n",
    "                    G.add_node(doctrine_node_id, node_type='doctrine', \n",
    "                              label=doctrine[:30], size=1700)\n",
    "                G.add_edge(case_node_id, doctrine_node_id, \n",
    "                          relation='applies_doctrine', weight=1.0)\n",
    "            for judge in entities.get('judges', [])[:3]:\n",
    "                judge_node_id = f\"Judge_{judge[:30]}\"\n",
    "                if judge_node_id not in G:\n",
    "                    G.add_node(judge_node_id, node_type='judge', \n",
    "                              label=judge[:30], size=2000)\n",
    "                G.add_edge(case_node_id, judge_node_id, relation='decided_by', weight=1.0)\n",
    "        logger.info(f\"{court_name} graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "        return G    \n",
    "    def create_unified_knowledge_graph(self, max_cases_per_court: Optional[int] = None,\n",
    "                                      batch_size: int = 500) -> nx.DiGraph:\n",
    "        logger.info(\"Creating unified knowledge graph with batch processing...\")\n",
    "        G = nx.DiGraph()\n",
    "        supreme_node = \"Supreme_Court_of_India\"\n",
    "        G.add_node(supreme_node, node_type='supreme_court',\n",
    "                  label='Supreme Court of India', hierarchy_level=0, size=10000)\n",
    "        high_courts = ['allahabad_high_court', 'bombay_high_court', 'calcutta_high_court',\n",
    "                      'delhi_high_court', 'madras_high_court']\n",
    "        for court in high_courts:\n",
    "            court_node = f\"Court_{court}\"\n",
    "            G.add_node(court_node, node_type='high_court',\n",
    "                      label=court.replace('_', ' ').title(), hierarchy_level=1, size=7000)\n",
    "            G.add_edge(supreme_node, court_node, relation='superior_to', weight=2.0)        \n",
    "        sc_node = \"Court_supreme_court\"\n",
    "        G.add_node(sc_node, node_type='supreme_court_cases',\n",
    "                  label='Supreme Court Cases', hierarchy_level=0, size=8000)\n",
    "        global_entities = {\n",
    "            'acts': {}, 'provisions': {}, 'concepts': {},\n",
    "            'doctrines': {}, 'parties': {}, 'judges': {}}\n",
    "        case_counter = 0\n",
    "        for court_name, court_cases in self.cases_data.items():\n",
    "            logger.info(f\"Processing {court_name}...\")\n",
    "            if max_cases_per_court and len(court_cases) > max_cases_per_court:\n",
    "                sampled_cases = random.sample(court_cases, max_cases_per_court)\n",
    "            else:\n",
    "                sampled_cases = court_cases\n",
    "            court_node = f\"Court_{court_name}\"\n",
    "            for batch_start in range(0, len(sampled_cases), batch_size):\n",
    "                batch_end = min(batch_start + batch_size, len(sampled_cases))\n",
    "                batch = sampled_cases[batch_start:batch_end]\n",
    "                logger.info(f\"  Batch {batch_start//batch_size + 1}: Processing cases {batch_start} to {batch_end}\")\n",
    "                for case in tqdm(batch, desc=f\"Batch {batch_start}-{batch_end}\"):\n",
    "                    case_counter += 1\n",
    "                    case_id = case.get('file_name', f\"{court_name}_{case_counter}\")\n",
    "                    if case_id not in self.entity_mappings:\n",
    "                        entities = self.entity_extraction(case)\n",
    "                        self.entity_mappings[case_id] = entities\n",
    "                    else:\n",
    "                        entities = self.entity_mappings[case_id]\n",
    "                    self._add_case_to_graph(G, case_id, case_counter, court_name, \n",
    "                                           entities, court_node, global_entities)\n",
    "                gc.collect()\n",
    "                logger.info(f\"  Memory: Nodes={G.number_of_nodes():,}, Edges={G.number_of_edges():,}\")\n",
    "        logger.info(\"Calculating graph metrics...\")\n",
    "        self._calculate_graph_metrics(G)\n",
    "        logger.info(f\"Unified graph: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "        self.global_knowledge_graph = G\n",
    "        return G\n",
    "    def _add_case_to_graph(self, G, case_id, case_counter, court_name, \n",
    "                           entities, court_node, global_entities):\n",
    "        case_node_id = f\"Case_{case_counter}\"\n",
    "        year = self._extract_year({'file_name': case_id, **entities})\n",
    "        case_types = entities.get('case_types', ['General'])\n",
    "        G.add_node(case_node_id, node_type='case', label=f\"Case {case_counter}\",\n",
    "                  case_id=case_id, court=court_name, year=year,\n",
    "                  case_types=case_types, size=2500)\n",
    "        G.add_edge(court_node, case_node_id, relation='adjudicated', weight=1.0)\n",
    "        for act_item in entities.get('acts', [])[:6]:\n",
    "            act_name = act_item['name'] if isinstance(act_item, dict) else act_item\n",
    "            act_key = act_name[:50]\n",
    "            if act_key not in global_entities['acts']:\n",
    "                act_node_id = f\"Act_{len(global_entities['acts'])}\"\n",
    "                global_entities['acts'][act_key] = act_node_id\n",
    "                G.add_node(act_node_id, node_type='act', label=act_name[:50],\n",
    "                          size=3500, citation_count=1)\n",
    "            else:\n",
    "                act_node_id = global_entities['acts'][act_key]\n",
    "                G.nodes[act_node_id]['citation_count'] += 1\n",
    "            G.add_edge(case_node_id, act_node_id, relation='governed_by', weight=1.0)\n",
    "        for prov_type, prov_list in entities['provisions'].items():\n",
    "            for provision in prov_list[:8]:\n",
    "                prov_details = entities.get('provision_details', {}).get(provision, {})\n",
    "                parent_act = prov_details.get('act', 'Unknown')\n",
    "                prov_key = f\"{parent_act}_{prov_type}_{provision[:30]}\"\n",
    "                if prov_key not in global_entities['provisions']:\n",
    "                    prov_node_id = f\"Prov_{len(global_entities['provisions'])}\"\n",
    "                    global_entities['provisions'][prov_key] = prov_node_id\n",
    "                    G.add_node(prov_node_id, node_type='provision',\n",
    "                              label=provision[:30], size=2000, citation_count=1,\n",
    "                              parent_act=parent_act)\n",
    "                else:\n",
    "                    prov_node_id = global_entities['provisions'][prov_key]\n",
    "                    G.nodes[prov_node_id]['citation_count'] += 1\n",
    "                G.add_edge(case_node_id, prov_node_id, relation='cites_provision', weight=1.0)\n",
    "        for concept in entities.get('legal_concepts', [])[:8]:\n",
    "            concept_key = concept[:40]\n",
    "            if concept_key not in global_entities['concepts']:\n",
    "                concept_node_id = f\"Concept_{len(global_entities['concepts'])}\"\n",
    "                global_entities['concepts'][concept_key] = concept_node_id\n",
    "                G.add_node(concept_node_id, node_type='concept',\n",
    "                          label=concept[:40], size=2200, mention_count=1)\n",
    "            else:\n",
    "                concept_node_id = global_entities['concepts'][concept_key]\n",
    "                G.nodes[concept_node_id]['mention_count'] += 1\n",
    "            G.add_edge(case_node_id, concept_node_id, relation='involves_concept', weight=1.0)\n",
    "        for doctrine in entities.get('doctrines', [])[:5]:\n",
    "            doctrine_key = doctrine[:40]\n",
    "            if doctrine_key not in global_entities['doctrines']:\n",
    "                doctrine_node_id = f\"Doctrine_{len(global_entities['doctrines'])}\"\n",
    "                global_entities['doctrines'][doctrine_key] = doctrine_node_id\n",
    "                G.add_node(doctrine_node_id, node_type='doctrine',\n",
    "                          label=doctrine[:40], size=2400, application_count=1)\n",
    "            else:\n",
    "                doctrine_node_id = global_entities['doctrines'][doctrine_key]\n",
    "                G.nodes[doctrine_node_id]['application_count'] += 1\n",
    "            G.add_edge(case_node_id, doctrine_node_id, relation='applies_doctrine', weight=1.0)\n",
    "        for judge in entities.get('judges', [])[:3]:\n",
    "            judge_key = judge[:40]\n",
    "            if judge_key not in global_entities['judges']:\n",
    "                judge_node_id = f\"Judge_{len(global_entities['judges'])}\"\n",
    "                global_entities['judges'][judge_key] = judge_node_id\n",
    "                G.add_node(judge_node_id, node_type='judge',\n",
    "                          label=judge[:40], size=2600, case_count=1)\n",
    "            else:\n",
    "                judge_node_id = global_entities['judges'][judge_key]\n",
    "                G.nodes[judge_node_id]['case_count'] += 1\n",
    "            G.add_edge(case_node_id, judge_node_id, relation='decided_by', weight=1.0)\n",
    "    def _calculate_graph_metrics(self, G: nx.DiGraph):\n",
    "        logger.info(\"Computing centrality measures...\")\n",
    "        degree_centrality = nx.degree_centrality(G)\n",
    "        for node, centrality in degree_centrality.items():\n",
    "            G.nodes[node]['degree_centrality'] = centrality\n",
    "        try:\n",
    "            max_iter = 30 if G.number_of_nodes() > 10000 else 50\n",
    "            pagerank = nx.pagerank(G, max_iter=max_iter, tol=1e-4)\n",
    "            for node, score in pagerank.items():\n",
    "                G.nodes[node]['pagerank'] = score\n",
    "        except:\n",
    "            logger.warning(\"PageRank calculation failed\")        \n",
    "        if G.number_of_nodes() < 3000:\n",
    "            try:\n",
    "                k_sample = min(50, G.number_of_nodes() // 10)\n",
    "                betweenness = nx.betweenness_centrality(G, k=k_sample)\n",
    "                for node, score in betweenness.items():\n",
    "                    G.nodes[node]['betweenness'] = score\n",
    "            except:\n",
    "                logger.warning(\"Betweenness calculation failed\")\n",
    "        else:\n",
    "            logger.info(\"Skipping betweenness centrality for large graph (>3000 nodes)\")\n",
    "    def visualize_knowledge_graph(self, G: nx.DiGraph, title: str, figsize=(28, 22)):\n",
    "        if G is None or G.number_of_nodes() == 0:\n",
    "            logger.warning(\"Empty graph, skipping visualization\")\n",
    "            return\n",
    "        logger.info(f\"Visualizing: {title}...\")\n",
    "        plt.figure(figsize=figsize)\n",
    "        color_map = {\n",
    "            'supreme_court': '#8B0000', 'high_court': '#DC143C',\n",
    "            'supreme_court_cases': '#B22222', 'court': '#E74C3C',\n",
    "            'case': '#3498DB', 'party': '#2ECC71', 'provision': '#9B59B6',\n",
    "            'act': '#F39C12', 'concept': '#1ABC9C', 'doctrine': '#E67E22',\n",
    "            'judge': '#34495E', 'cited_case': '#95A5A6', 'rulebook': '#8E44AD'}\n",
    "        node_colors = [color_map.get(G.nodes[node].get('node_type', 'case'), '#95A5A6')\n",
    "                      for node in G.nodes()]\n",
    "        node_sizes = [G.nodes[node].get('size', 1500) for node in G.nodes()]\n",
    "        logger.info(\"Computing layout...\")\n",
    "        if G.number_of_nodes() < 500:\n",
    "            pos = nx.spring_layout(G, k=3, iterations=50, seed=42, scale=3)\n",
    "        else:\n",
    "            pos = nx.kamada_kawai_layout(G, scale=3)        \n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes,\n",
    "                              alpha=0.85, edgecolors='black', linewidths=1.5)\n",
    "        if G.number_of_nodes() < 300:\n",
    "            labels = {node: G.nodes[node].get('label', str(node))[:25] for node in G.nodes()}\n",
    "            nx.draw_networkx_labels(G, pos, labels, font_size=6, font_weight='bold')\n",
    "        else:\n",
    "            important_nodes = [n for n in G.nodes() \n",
    "                             if G.nodes[n].get('node_type') in ['supreme_court', 'high_court', 'court']]\n",
    "            labels = {node: G.nodes[node].get('label', str(node))[:25] for node in important_nodes}\n",
    "            nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight='bold')\n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.2, arrows=True, arrowsize=8,\n",
    "                              edge_color='#34495E', width=0.8)\n",
    "        legend_elements = [plt.Line2D([0], [0], marker='o', color='w',\n",
    "                          markerfacecolor=color, markersize=12,\n",
    "                          label=ntype.replace('_', ' ').title())\n",
    "                          for ntype, color in color_map.items()]\n",
    "        plt.legend(handles=legend_elements, loc='upper left', fontsize=10, ncol=2)\n",
    "        stats_text = f\"{title}\\nNodes: {G.number_of_nodes()} | Edges: {G.number_of_edges()}\"\n",
    "        plt.title(stats_text, fontsize=18, fontweight='bold', pad=20)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    def visualize_hierarchical_graph(self, G: nx.DiGraph, title: str, figsize=(30, 24)):\n",
    "        if G is None or G.number_of_nodes() == 0:\n",
    "            logger.warning(\"Empty graph, skipping visualization\")\n",
    "            return\n",
    "        logger.info(f\"Creating hierarchical visualization: {title}...\")\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        color_map = {\n",
    "            'supreme_court': '#8B0000', 'high_court': '#DC143C',\n",
    "            'supreme_court_cases': '#B22222', 'court': '#E74C3C',\n",
    "            'case': '#3498DB', 'party': '#2ECC71', 'provision': '#9B59B6',\n",
    "            'act': '#F39C12', 'concept': '#1ABC9C', 'doctrine': '#E67E22',\n",
    "            'judge': '#34495E', 'cited_case': '#95A5A6', 'rulebook': '#8E44AD'}        \n",
    "        edge_color_map = {\n",
    "            'adjudicated': '#E74C3C', 'decided_by': '#34495E',\n",
    "            'governed_by': '#F39C12', 'cites_provision': '#9B59B6',\n",
    "            'involves_concept': '#1ABC9C', 'applies_doctrine': '#E67E22',\n",
    "            'references_rulebook': '#8E44AD', 'defined_in': '#6C3483',\n",
    "            'codified_in': '#7D3C98', 'superior_to': '#8B0000'}\n",
    "        logger.info(\"Computing hierarchical layout...\")\n",
    "        pos = self._compute_hierarchical_layout(G)\n",
    "        node_colors = [color_map.get(G.nodes[node].get('node_type', 'case'), '#95A5A6')\n",
    "                      for node in G.nodes()]\n",
    "        node_sizes = [G.nodes[node].get('size', 1500) for node in G.nodes()]        \n",
    "        edge_types = defaultdict(list)\n",
    "        for u, v, data in G.edges(data=True):\n",
    "            relation = data.get('relation', 'unknown')\n",
    "            edge_types[relation].append((u, v))\n",
    "        for relation, edges in edge_types.items():\n",
    "            color = edge_color_map.get(relation, '#34495E')\n",
    "            alpha = 0.4 if relation in ['adjudicated', 'governed_by'] else 0.2\n",
    "            width = 1.5 if relation in ['adjudicated', 'governed_by'] else 0.8\n",
    "            nx.draw_networkx_edges(G, pos, edgelist=edges, edge_color=color,\n",
    "                                  alpha=alpha, arrows=True, arrowsize=10,\n",
    "                                  width=width, ax=ax)        \n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes,\n",
    "                              alpha=0.9, edgecolors='black', linewidths=2, ax=ax)        \n",
    "        if G.number_of_nodes() < 200:\n",
    "            labels = {node: G.nodes[node].get('label', str(node))[:20] for node in G.nodes()}\n",
    "            nx.draw_networkx_labels(G, pos, labels, font_size=7, font_weight='bold', ax=ax)\n",
    "        else:\n",
    "            important_types = ['supreme_court', 'high_court', 'court', 'supreme_court_cases']\n",
    "            important_nodes = [n for n in G.nodes() if G.nodes[n].get('node_type') in important_types]\n",
    "            labels = {node: G.nodes[node].get('label', str(node))[:20] for node in important_nodes}\n",
    "            nx.draw_networkx_labels(G, pos, labels, font_size=8, font_weight='bold', ax=ax)        \n",
    "        node_legend = [plt.Line2D([0], [0], marker='o', color='w',\n",
    "                      markerfacecolor=color, markersize=12,\n",
    "                      label=ntype.replace('_', ' ').title())\n",
    "                      for ntype, color in color_map.items()]\n",
    "        edge_legend = [plt.Line2D([0], [0], color=color, linewidth=2,\n",
    "                      label=relation.replace('_', ' ').title())\n",
    "                      for relation, color in list(edge_color_map.items())[:8]]\n",
    "        first_legend = ax.legend(handles=node_legend, loc='upper left', fontsize=9,\n",
    "                                title='Node Types', title_fontsize=10)\n",
    "        ax.add_artist(first_legend)\n",
    "        ax.legend(handles=edge_legend, loc='upper right', fontsize=9,\n",
    "                 title='Relationships', title_fontsize=10)\n",
    "        stats_text = f\"{title} (Hierarchical Layout)\\nNodes: {G.number_of_nodes()} | Edges: {G.number_of_edges()}\"\n",
    "        ax.set_title(stats_text, fontsize=18, fontweight='bold', pad=20)\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    def _compute_hierarchical_layout(self, G: nx.DiGraph) -> Dict:\n",
    "        pos = {}        \n",
    "        hierarchy_levels = {\n",
    "            'supreme_court': 0, 'supreme_court_cases': 1, 'high_court': 1,\n",
    "            'court': 2, 'rulebook': 2, 'case': 3, 'judge': 4, 'act': 4,\n",
    "            'provision': 5, 'doctrine': 5, 'concept': 5, 'party': 6, 'cited_case': 6}\n",
    "        levels = defaultdict(list)\n",
    "        for node in G.nodes():\n",
    "            node_type = G.nodes[node].get('node_type', 'case')\n",
    "            level = hierarchy_levels.get(node_type, 5)\n",
    "            levels[level].append(node)\n",
    "        y_spacing = 1.5\n",
    "        max_level = max(levels.keys()) if levels else 0\n",
    "        for level, nodes in levels.items():\n",
    "            y = (max_level - level) * y_spacing\n",
    "            x_spacing = 10.0 / max(len(nodes), 1)\n",
    "            for i, node in enumerate(nodes):\n",
    "                x = (i - len(nodes) / 2) * x_spacing\n",
    "                pos[node] = (x, y)\n",
    "        return pos    \n",
    "    def visualize_graph_statistics_panel(self, G: nx.DiGraph, title: str):\n",
    "        logger.info(f\"Creating statistics panel: {title}...\")\n",
    "        fig = plt.figure(figsize=(24, 16))\n",
    "        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)        \n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        node_types = [G.nodes[n].get('node_type', 'unknown') for n in G.nodes()]\n",
    "        type_counts = Counter(node_types)\n",
    "        colors_list = ['#3498DB', '#E74C3C', '#F39C12', '#9B59B6', '#1ABC9C', \n",
    "                      '#2ECC71', '#E67E22', '#34495E', '#95A5A6']\n",
    "        ax1.pie(type_counts.values(), labels=[t.replace('_', ' ').title() for t in type_counts.keys()],\n",
    "               autopct='%1.1f%%', colors=colors_list[:len(type_counts)], startangle=90)\n",
    "        ax1.set_title('Node Type Distribution', fontsize=12, fontweight='bold')        \n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        edge_types = [G.edges[e].get('relation', 'unknown') for e in G.edges()]\n",
    "        edge_counts = Counter(edge_types)\n",
    "        top_edges = dict(edge_counts.most_common(8))\n",
    "        ax2.barh(list(top_edges.keys()), list(top_edges.values()), color='#3498DB')\n",
    "        ax2.set_xlabel('Count', fontsize=10)\n",
    "        ax2.set_title('Top Edge Types', fontsize=12, fontweight='bold')\n",
    "        ax2.invert_yaxis()            \n",
    "        ax9 = fig.add_subplot(gs[0,2])\n",
    "        case_nodes = [n for n in G.nodes() if G.nodes[n].get('node_type') == 'case']\n",
    "        if case_nodes:\n",
    "            case_types = []\n",
    "            for node in case_nodes:\n",
    "                types = G.nodes[node].get('case_types', ['General'])\n",
    "                case_types.extend(types)\n",
    "            type_counts = Counter(case_types)\n",
    "            if type_counts:\n",
    "                top_types = dict(type_counts.most_common(8))\n",
    "                colors_case = plt.cm.Set3(range(len(top_types)))\n",
    "                ax9.pie(top_types.values(), labels=top_types.keys(),\n",
    "                       autopct='%1.1f%%', colors=colors_case, startangle=90)\n",
    "                ax9.set_title('Case Type Distribution', fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            ax9.text(0.5, 0.5, 'No case data', ha='center', va='center')\n",
    "            ax9.set_title('Case Type Distribution', fontsize=12, fontweight='bold')\n",
    "        fig.suptitle(f'{title} - Comprehensive Statistics Dashboard',\n",
    "                    fontsize=18, fontweight='bold', y=0.98)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    def visualize_subgraph_citation_network(self, G: nx.DiGraph, title: str, figsize=(24, 18)):\n",
    "        logger.info(f\"Creating citation network subgraph: {title}...\")\n",
    "        citation_edges = [(u, v) for u, v, data in G.edges(data=True)\n",
    "                         if data.get('relation') in ['cites_case', 'cites_provision']]\n",
    "        if not citation_edges:\n",
    "            logger.warning(\"No citation edges found\")\n",
    "            return\n",
    "        citation_nodes = set()\n",
    "        for u, v in citation_edges:\n",
    "            citation_nodes.add(u)\n",
    "            citation_nodes.add(v)\n",
    "        subG = G.subgraph(citation_nodes).copy()\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        color_map = {'case': '#3498DB', 'cited_case': '#95A5A6', 'provision': '#9B59B6'}\n",
    "        node_colors = [color_map.get(subG.nodes[node].get('node_type', 'case'), '#95A5A6')\n",
    "                      for node in subG.nodes()]\n",
    "        node_sizes = [subG.nodes[node].get('size', 1500) * 1.2 for node in subG.nodes()]\n",
    "        pos = nx.spring_layout(subG, k=2, iterations=50, seed=42)\n",
    "        nx.draw_networkx_nodes(subG, pos, node_color=node_colors, node_size=node_sizes,\n",
    "                              alpha=0.85, edgecolors='black', linewidths=2, ax=ax)\n",
    "        nx.draw_networkx_edges(subG, pos, alpha=0.4, arrows=True, arrowsize=12,\n",
    "                              edge_color='#8E44AD', width=1.5, ax=ax)\n",
    "        if len(subG.nodes()) < 100:\n",
    "            labels = {node: subG.nodes[node].get('label', str(node))[:20] for node in subG.nodes()}\n",
    "            nx.draw_networkx_labels(subG, pos, labels, font_size=7, font_weight='bold', ax=ax)\n",
    "        ax.set_title(f\"{title} - Citation Network\\nNodes: {subG.number_of_nodes()} | Links: {subG.number_of_edges()}\",\n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    def visualize_subgraph_statutory_framework(self, G: nx.DiGraph, title: str, figsize=(24, 18)):\n",
    "        logger.info(f\"Creating statutory framework subgraph: {title}...\")\n",
    "        statutory_nodes = [n for n in G.nodes() if G.nodes[n].get('node_type') in ['act', 'provision', 'case']]\n",
    "        if not statutory_nodes:\n",
    "            logger.warning(\"No statutory nodes found\")\n",
    "            return\n",
    "        subG = G.subgraph(statutory_nodes).copy()\n",
    "        edges_to_remove = []\n",
    "        for u, v, data in subG.edges(data=True):\n",
    "            if data.get('relation') not in ['governed_by', 'cites_provision']:\n",
    "                edges_to_remove.append((u, v))\n",
    "        subG.remove_edges_from(edges_to_remove)\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        color_map = {'act': '#F39C12', 'provision': '#9B59B6', 'case': '#3498DB'}\n",
    "        node_colors = [color_map.get(subG.nodes[node].get('node_type', 'case'), '#95A5A6')\n",
    "                      for node in subG.nodes()]\n",
    "        node_sizes = [subG.nodes[node].get('size', 1500) * 1.2 for node in subG.nodes()]\n",
    "        pos = self._compute_hierarchical_layout(subG)\n",
    "        nx.draw_networkx_nodes(subG, pos, node_color=node_colors, node_size=node_sizes,\n",
    "                              alpha=0.85, edgecolors='black', linewidths=2, ax=ax)\n",
    "        governed_edges = [(u, v) for u, v, d in subG.edges(data=True) if d.get('relation') == 'governed_by']\n",
    "        citation_edges = [(u, v) for u, v, d in subG.edges(data=True) if d.get('relation') == 'cites_provision']\n",
    "        nx.draw_networkx_edges(subG, pos, edgelist=governed_edges, alpha=0.5,\n",
    "                              arrows=True, arrowsize=12, edge_color='#E67E22',\n",
    "                              width=2, ax=ax, label='Governed By')\n",
    "        nx.draw_networkx_edges(subG, pos, edgelist=citation_edges, alpha=0.4,\n",
    "                              arrows=True, arrowsize=10, edge_color='#8E44AD',\n",
    "                              width=1.5, ax=ax, label='Cites Provision')\n",
    "        if len(subG.nodes()) < 100:\n",
    "            labels = {node: subG.nodes[node].get('label', str(node))[:25] for node in subG.nodes()}\n",
    "            nx.draw_networkx_labels(subG, pos, labels, font_size=7, font_weight='bold', ax=ax)\n",
    "        ax.set_title(f\"{title} - Statutory Framework\\nNodes: {subG.number_of_nodes()} | Relationships: {subG.number_of_edges()}\",\n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.legend(fontsize=12, loc='upper right')\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    def visualize_subgraph_concept_doctrine_network(self, G: nx.DiGraph, title: str, figsize=(24, 18)):\n",
    "        logger.info(f\"Creating concept-doctrine network subgraph: {title}...\")\n",
    "        concept_nodes = [n for n in G.nodes() if G.nodes[n].get('node_type') in ['concept', 'doctrine', 'case']]\n",
    "        if not concept_nodes:\n",
    "            logger.warning(\"No concept/doctrine nodes found\")\n",
    "            return\n",
    "        subG = G.subgraph(concept_nodes).copy()\n",
    "        edges_to_keep = []\n",
    "        for u, v, data in subG.edges(data=True):\n",
    "            relation = data.get('relation', '')\n",
    "            if relation in ['involves_concept', 'applies_doctrine']:\n",
    "                edges_to_keep.append((u, v))\n",
    "        filtered_G = nx.DiGraph()\n",
    "        filtered_G.add_nodes_from(subG.nodes(data=True))\n",
    "        filtered_G.add_edges_from([(u, v, subG[u][v]) for u, v in edges_to_keep])\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        color_map = {'concept': '#1ABC9C', 'doctrine': '#E67E22', 'case': '#3498DB'}\n",
    "        node_colors = [color_map.get(filtered_G.nodes[node].get('node_type', 'concept'), '#95A5A6')\n",
    "                      for node in filtered_G.nodes()]\n",
    "        node_sizes = [filtered_G.nodes[node].get('size', 1500) * 1.2 for node in filtered_G.nodes()]\n",
    "        pos = nx.spring_layout(filtered_G, k=2.5, iterations=50, seed=42)\n",
    "        nx.draw_networkx_nodes(filtered_G, pos, node_color=node_colors, node_size=node_sizes,\n",
    "                              alpha=0.85, edgecolors='black', linewidths=2, ax=ax)\n",
    "        nx.draw_networkx_edges(filtered_G, pos, alpha=0.3, arrows=True, arrowsize=10,\n",
    "                              edge_color='#16A085', width=1.2, ax=ax)\n",
    "        if len(filtered_G.nodes()) < 100:\n",
    "            labels = {node: filtered_G.nodes[node].get('label', str(node))[:25] for node in filtered_G.nodes()}\n",
    "            nx.draw_networkx_labels(filtered_G, pos, labels, font_size=8, font_weight='bold', ax=ax)\n",
    "        legend_elements = [plt.Line2D([0], [0], marker='o', color='w',\n",
    "                          markerfacecolor=color, markersize=14, label=ntype.title())\n",
    "                          for ntype, color in color_map.items()]\n",
    "        ax.legend(handles=legend_elements, fontsize=12, loc='upper right')\n",
    "        ax.set_title(f\"{title} - Legal Concepts & Doctrines\\nNodes: {filtered_G.number_of_nodes()} | Relationships: {filtered_G.number_of_edges()}\",\n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    def visualize_rulebook_integration_graph(self, G: nx.DiGraph, title: str, figsize=(26, 20)):\n",
    "        logger.info(f\"Creating rulebook integration graph: {title}...\")\n",
    "        rulebook_nodes = [n for n in G.nodes() if G.nodes[n].get('node_type') in \n",
    "                         ['rulebook', 'case', 'act', 'provision']]\n",
    "        if not rulebook_nodes:\n",
    "            logger.warning(\"No rulebook nodes found\")\n",
    "            return\n",
    "        subG = G.subgraph(rulebook_nodes).copy()\n",
    "        edges_to_keep = []\n",
    "        for u, v, data in subG.edges(data=True):\n",
    "            relation = data.get('relation', '')\n",
    "            if relation in ['references_rulebook', 'defined_in', 'codified_in', 'governed_by', 'cites_provision']:\n",
    "                edges_to_keep.append((u, v))\n",
    "        filtered_G = nx.DiGraph()\n",
    "        filtered_G.add_nodes_from(subG.nodes(data=True))\n",
    "        filtered_G.add_edges_from([(u, v, subG[u][v]) for u, v in edges_to_keep])\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        color_map = {'rulebook': '#8E44AD', 'case': '#3498DB', 'act': '#F39C12', 'provision': '#9B59B6'}\n",
    "        node_colors = [color_map.get(filtered_G.nodes[node].get('node_type', 'case'), '#95A5A6')\n",
    "                      for node in filtered_G.nodes()]\n",
    "        node_sizes = []\n",
    "        for node in filtered_G.nodes():\n",
    "            base_size = filtered_G.nodes[node].get('size', 2000)\n",
    "            if filtered_G.nodes[node].get('validated', False):\n",
    "                node_sizes.append(base_size * 1.3)\n",
    "            else:\n",
    "                node_sizes.append(base_size)\n",
    "        pos = self._compute_hierarchical_layout(filtered_G)\n",
    "        edge_types = {\n",
    "            'references_rulebook': [], 'defined_in': [], 'codified_in': [],\n",
    "            'governed_by': [], 'cites_provision': []}\n",
    "        for u, v, data in filtered_G.edges(data=True):\n",
    "            relation = data.get('relation', 'unknown')\n",
    "            if relation in edge_types:\n",
    "                edge_types[relation].append((u, v))\n",
    "        edge_colors = {\n",
    "            'references_rulebook': '#8E44AD', 'defined_in': '#6C3483',\n",
    "            'codified_in': '#7D3C98', 'governed_by': '#F39C12', 'cites_provision': '#9B59B6'}\n",
    "        for relation, edges in edge_types.items():\n",
    "            if edges:\n",
    "                nx.draw_networkx_edges(filtered_G, pos, edgelist=edges,\n",
    "                                      edge_color=edge_colors[relation], alpha=0.6,\n",
    "                                      arrows=True, arrowsize=12, width=2.5 if 'rulebook' in relation else 1.5,\n",
    "                                      ax=ax, label=relation.replace('_', ' ').title())\n",
    "        nx.draw_networkx_nodes(filtered_G, pos, node_color=node_colors, node_size=node_sizes,\n",
    "                              alpha=0.9, edgecolors='black', linewidths=2, ax=ax)\n",
    "        if len(filtered_G.nodes()) < 150:\n",
    "            labels = {node: filtered_G.nodes[node].get('label', str(node))[:30] for node in filtered_G.nodes()}\n",
    "            nx.draw_networkx_labels(filtered_G, pos, labels, font_size=7, font_weight='bold', ax=ax)\n",
    "        else:\n",
    "            important_nodes = [n for n in filtered_G.nodes() \n",
    "                             if filtered_G.nodes[n].get('node_type') in ['rulebook', 'case']]\n",
    "            labels = {node: filtered_G.nodes[node].get('label', str(node))[:30] for node in important_nodes}\n",
    "            nx.draw_networkx_labels(filtered_G, pos, labels, font_size=8, font_weight='bold', ax=ax)\n",
    "        node_legend = [plt.Line2D([0], [0], marker='o', color='w',\n",
    "                      markerfacecolor=color, markersize=14, label=ntype.title())\n",
    "                      for ntype, color in color_map.items()]        \n",
    "        first_legend = ax.legend(handles=node_legend, loc='upper left', fontsize=10,\n",
    "                                title='Node Types', title_fontsize=11)\n",
    "        ax.add_artist(first_legend)\n",
    "        ax.legend(fontsize=10, loc='upper right', title='Relationships', title_fontsize=11)\n",
    "        ax.set_title(f\"{title} - Rulebook Integration\\nNodes: {filtered_G.number_of_nodes()} | \"\n",
    "                    f\"Relationships: {filtered_G.number_of_edges()}\",\n",
    "                    fontsize=16, fontweight='bold', pad=20)\n",
    "        ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    def export_for_gnn(self, output_dir: str = \"gnn_data\"):\n",
    "        logger.info(\"Exporting data for GNN...\")        \n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(exist_ok=True)\n",
    "        G = self.global_knowledge_graph\n",
    "        node_list = list(G.nodes())\n",
    "        node_to_idx = {node: idx for idx, node in enumerate(node_list)}\n",
    "        node_features = []\n",
    "        node_metadata = []\n",
    "        for node in node_list:\n",
    "            attrs = G.nodes[node]\n",
    "            features = [\n",
    "                attrs.get('size', 1500) / 10000, attrs.get('degree_centrality', 0),\n",
    "                attrs.get('pagerank', 0), attrs.get('betweenness', 0), attrs.get('citation_count', 0) / 100,\n",
    "                attrs.get('mention_count', 0) / 100, attrs.get('hierarchy_level', 2) / 2,]\n",
    "            node_features.append(features)\n",
    "            node_metadata.append({\n",
    "                'id': node, 'label': attrs.get('label', ''),\n",
    "                'type': attrs.get('node_type', ''),\n",
    "                'attributes': {k: v for k, v in attrs.items() if k != 'label'}})\n",
    "        np.save(output_path / 'node_features.npy', np.array(node_features))\n",
    "        with open(output_path / 'node_metadata.json', 'w') as f:\n",
    "            json.dump(node_metadata, f, indent=2)\n",
    "        edge_list = []\n",
    "        edge_features = []\n",
    "        edge_types = []\n",
    "        for u, v, attrs in G.edges(data=True):\n",
    "            edge_list.append([node_to_idx[u], node_to_idx[v]])\n",
    "            features = [\n",
    "                attrs.get('weight', 1.0),\n",
    "                1 if attrs.get('relation') == 'cites_provision' else 0,\n",
    "                1 if attrs.get('relation') == 'superior_to' else 0,\n",
    "                1 if attrs.get('relation') == 'governed_by' else 0,]\n",
    "            edge_features.append(features)\n",
    "            edge_types.append(attrs.get('relation', 'unknown'))\n",
    "        np.save(output_path / 'edge_list.npy', np.array(edge_list))\n",
    "        np.save(output_path / 'edge_features.npy', np.array(edge_features))\n",
    "        with open(output_path / 'edge_types.json', 'w') as f:\n",
    "            json.dump(edge_types, f)\n",
    "        stats = {\n",
    "            'num_nodes': G.number_of_nodes(), 'num_edges': G.number_of_edges(),\n",
    "            'node_type_distribution': dict(Counter([G.nodes[n].get('node_type', 'unknown') for n in G.nodes()])),\n",
    "            'edge_type_distribution': dict(Counter(edge_types)),\n",
    "            'avg_degree': sum(dict(G.degree()).values()) / max(G.number_of_nodes(), 1),}\n",
    "        with open(output_path / 'graph_stats.json', 'w') as f:\n",
    "            json.dump(stats, f, indent=2)        \n",
    "        try:\n",
    "            import pickle\n",
    "            with open(output_path / 'knowledge_graph.gpickle', 'wb') as f:\n",
    "                pickle.dump(G, f)\n",
    "            logger.info(\"Graph saved as knowledge_graph.gpickle\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to save graph with pickle: {e}\")\n",
    "        try:\n",
    "            self._export_pyg_format(output_path, node_to_idx)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"PyG export failed: {e}\")\n",
    "        logger.info(f\"GNN data exported to {output_path}\")\n",
    "    def _export_pyg_format(self, output_path: Path, node_to_idx: Dict):\n",
    "        try:\n",
    "            import torch\n",
    "            from torch_geometric.data import Data            \n",
    "            G = self.global_knowledge_graph\n",
    "            x = torch.tensor(np.load(output_path / 'node_features.npy'), dtype=torch.float)\n",
    "            edge_index = torch.tensor(np.load(output_path / 'edge_list.npy').T, dtype=torch.long)\n",
    "            edge_attr = torch.tensor(np.load(output_path / 'edge_features.npy'), dtype=torch.float)\n",
    "            data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "            torch.save(data, output_path / 'pyg_data.pt')\n",
    "            logger.info(\"PyTorch Geometric format exported\")\n",
    "        except ImportError:\n",
    "            logger.warning(\"PyTorch or PyTorch Geometric not installed, skipping\")\n",
    "    def generate_comprehensive_report(self):\n",
    "        logger.info(\"\\n\")\n",
    "        logger.info(\"Knowledge Graph Analysis Report:\")        \n",
    "        logger.info(f\"\\nOverall Statistics:\")\n",
    "        logger.info(f\"Total Cases: {self.stats['total_cases']:,}\")\n",
    "        logger.info(f\"Cases By Court:\")\n",
    "        for court, count in sorted(self.stats['by_court'].items()):\n",
    "            logger.info(f\"  {court.replace('_', ' ').title()}: {count:,}\")\n",
    "        logger.info(f\"Cases By Year (Top 10):\")\n",
    "        for year, count in Counter(self.stats['by_year']).most_common(10):\n",
    "            logger.info(f\"  {year}: {count:,}\")        \n",
    "        if self.global_knowledge_graph:\n",
    "            G = self.global_knowledge_graph\n",
    "            logger.info(f\"\\nUnified Knowledge Graph:\")\n",
    "            logger.info(f\"  Total Nodes: {G.number_of_nodes():,}\")\n",
    "            logger.info(f\"  Total Edges: {G.number_of_edges():,}\")\n",
    "            logger.info(f\"  Avg Degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")\n",
    "            logger.info(f\"\\nNode Type Distribution:\")\n",
    "            node_types = Counter([G.nodes[n].get('node_type', 'unknown') for n in G.nodes()])\n",
    "            for ntype, count in node_types.most_common():\n",
    "                logger.info(f\"  {ntype.replace('_', ' ').title()}: {count:,}\")\n",
    "            logger.info(f\"\\nEdge Type Distribution:\")\n",
    "            edge_types = Counter([G.edges[e].get('relation', 'unknown') for e in G.edges()])\n",
    "            for etype, count in edge_types.most_common():\n",
    "                logger.info(f\"  {etype.replace('_', ' ').title()}: {count:,}\")            \n",
    "            logger.info(f\"\\nTop Cited Acts:\")\n",
    "            act_nodes = [n for n in G.nodes() if G.nodes[n].get('node_type') == 'act']\n",
    "            top_acts = sorted(act_nodes, key=lambda n: G.nodes[n].get('citation_count', 0), reverse=True)[:10]\n",
    "            for act in top_acts:\n",
    "                count = G.nodes[act].get('citation_count', 0)\n",
    "                label = G.nodes[act].get('label', act)\n",
    "                logger.info(f\"  {label}: {count} citations\")\n",
    "            logger.info(f\"\\nTop Cited Provisions:\")\n",
    "            prov_nodes = [n for n in G.nodes() if G.nodes[n].get('node_type') == 'provision']\n",
    "            top_provs = sorted(prov_nodes, key=lambda n: G.nodes[n].get('citation_count', 0), reverse=True)[:10]\n",
    "            for prov in top_provs:\n",
    "                count = G.nodes[prov].get('citation_count', 0)\n",
    "                label = G.nodes[prov].get('label', prov)\n",
    "                validated = \" \" if G.nodes[prov].get('validated', False) else \"\"\n",
    "                parent_act = G.nodes[prov].get('parent_act', '')\n",
    "                rulebook = G.nodes[prov].get('rulebook', '')\n",
    "                if parent_act and parent_act != 'Unknown':\n",
    "                    logger.info(f\"  {label}{validated}: {count} citations [{parent_act}]\")\n",
    "                elif rulebook:\n",
    "                    logger.info(f\"  {label}{validated}: {count} citations [{rulebook}]\")\n",
    "                else:\n",
    "                    logger.info(f\"  {label}{validated}: {count} citations\")            \n",
    "            logger.info(f\"\\nRulebook Integration Statistics:\")\n",
    "            rulebook_stats = defaultdict(int)\n",
    "            validated_provisions = 0\n",
    "            total_provisions = 0\n",
    "            for node in G.nodes():\n",
    "                if G.nodes[node].get('node_type') == 'provision':\n",
    "                    total_provisions += 1\n",
    "                    if G.nodes[node].get('validated', False):\n",
    "                        validated_provisions += 1\n",
    "                        rulebook = G.nodes[node].get('rulebook', 'Unknown')\n",
    "                        rulebook_stats[rulebook] += 1\n",
    "            logger.info(f\"  Total Provisions Extracted: {total_provisions}\")\n",
    "            if rulebook_stats:\n",
    "                logger.info(f\"\\n  Provisions by Rulebook:\")\n",
    "                for rulebook, count in sorted(rulebook_stats.items(), key=lambda x: x[1], reverse=True):\n",
    "                    logger.info(f\"    {rulebook}: {count}\")\n",
    "        logger.info(\"\\n\")\n",
    "    def step1_setup(self):\n",
    "        logger.info(\"\\n\")\n",
    "        logger.info(\"Step 1: Setup and Data Loading -\")        \n",
    "        self.load_official_documents()\n",
    "        self.load_all_cases()\n",
    "        logger.info(\"\\nStep 1 Completed.\")\n",
    "        logger.info(f\"   Loaded {self.stats['total_cases']:,} cases from {len(self.cases_data)} courts\")\n",
    "        logger.info(f\"   Indexed {sum(len(v) for v in self.provision_index.values())} provisions from rulebooks\")\n",
    "    def step2_create_individual_graphs(self, num_cases_per_court: int = 10):\n",
    "        logger.info(\"\\n\")\n",
    "        logger.info(\"Step 2: Creating Individual Court Knowledge Graphs -\")\n",
    "        logger.info(f\"Sampling {num_cases_per_court} random cases from each court...\")\n",
    "        courts = [\n",
    "            'supreme_court', 'allahabad_high_court', 'bombay_high_court',\n",
    "            'calcutta_high_court', 'delhi_high_court', 'madras_high_court']\n",
    "        self.court_graphs = {}\n",
    "        for court in courts:\n",
    "            logger.info(\"\\n\")\n",
    "            logger.info(f\"Processing: {court.upper()}\")\n",
    "            G = self.create_court_knowledge_graph(court, num_cases=num_cases_per_court, random_sample=True)\n",
    "            self.court_graphs[court] = G\n",
    "            logger.info(\"\\n[1/7] Creating circular layout visualization...\")\n",
    "            self.visualize_knowledge_graph(G, f\"{court.replace('_', ' ').title()} - {num_cases_per_court} Random Cases\",\n",
    "                                          figsize=(26, 20))\n",
    "            logger.info(\"[2/7] Creating hierarchical layout visualization...\")\n",
    "            self.visualize_hierarchical_graph(G, f\"{court.replace('_', ' ').title()} - {num_cases_per_court} Random Cases\",\n",
    "                                             figsize=(28, 22))\n",
    "            logger.info(\"[3/7] Creating statistics dashboard...\")\n",
    "            self.visualize_graph_statistics_panel(G, court.replace('_', ' ').title())\n",
    "            logger.info(\"[4/7] Creating citation network subgraph...\")\n",
    "            self.visualize_subgraph_citation_network(G, court.replace('_', ' ').title())\n",
    "            logger.info(\"[5/7] Creating statutory framework subgraph...\")\n",
    "            self.visualize_subgraph_statutory_framework(G, court.replace('_', ' ').title())\n",
    "            logger.info(\"[6/7] Creating concept-doctrine network subgraph...\")\n",
    "            self.visualize_subgraph_concept_doctrine_network(G, court.replace('_', ' ').title())\n",
    "            logger.info(\"[7/7] Creating rulebook integration subgraph...\")\n",
    "            self.visualize_rulebook_integration_graph(G, court.replace('_', ' ').title())\n",
    "            logger.info(f\"Completed {court}: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "            gc.collect()\n",
    "        logger.info(\"\\n\")\n",
    "        logger.info(\"Step 2 Completed.\")\n",
    "        logger.info(f\"   Created {len(self.court_graphs)} court-specific knowledge graphs\")\n",
    "        logger.info(f\"   Generated {len(self.court_graphs) * 7} visualizations\")\n",
    "    def step3_create_unified_graph(self, max_cases_per_court: Optional[int] = None,\n",
    "                                   skip_large_visualizations: bool = False):\n",
    "        logger.info(\"\\n\")\n",
    "        logger.info(\"Step 3: Creating Unified Knowledge Graph -\")        \n",
    "        if max_cases_per_court:\n",
    "            logger.warning(f\"Limited to {max_cases_per_court} cases per court\")\n",
    "            logger.warning(f\"Total cases: ~{max_cases_per_court * 6} (instead of {self.stats['total_cases']:,})\")\n",
    "        else:\n",
    "            logger.info(f\"Processing all {self.stats['total_cases']:,} cases...\")        \n",
    "        G_unified = self.create_unified_knowledge_graph(max_cases_per_court=max_cases_per_court)\n",
    "        logger.info(\"\\n\")\n",
    "        logger.info(\"Visualizing Unified Graph...\")\n",
    "        if skip_large_visualizations and G_unified.number_of_nodes() > 10000:\n",
    "            logger.warning(f\"Graph has {G_unified.number_of_nodes():,} nodes, skipping large visualizations\")\n",
    "            logger.info(\"Generating statistics dashboard only...\")\n",
    "            self.visualize_graph_statistics_panel(G_unified, \"Unified Graph (All Courts)\")\n",
    "            logger.info(\"Creating focused subgraph visualizations...\")\n",
    "            self.visualize_rulebook_integration_graph(G_unified, \"Unified Graph\")\n",
    "        else:\n",
    "            logger.info(\"\\n[1/7] Creating circular layout...\")\n",
    "            self.visualize_knowledge_graph(G_unified, \"Unified Indian Legal Knowledge Graph (All Courts)\",\n",
    "                                          figsize=(32, 26))\n",
    "            logger.info(\"[2/7] Creating hierarchical layout...\")\n",
    "            self.visualize_hierarchical_graph(G_unified, \"Unified Indian Legal Knowledge Graph (All Courts)\",\n",
    "                                             figsize=(34, 28))\n",
    "            logger.info(\"[3/7] Creating statistics dashboard...\")\n",
    "            self.visualize_graph_statistics_panel(G_unified, \"Unified Graph (All Courts)\")\n",
    "            logger.info(\"[4/7] Creating unified citation network...\")\n",
    "            self.visualize_subgraph_citation_network(G_unified, \"Unified Graph\")\n",
    "            logger.info(\"[5/7] Creating unified statutory framework...\")\n",
    "            self.visualize_subgraph_statutory_framework(G_unified, \"Unified Graph\")\n",
    "            logger.info(\"[6/7] Creating unified concept-doctrine network...\")\n",
    "            self.visualize_subgraph_concept_doctrine_network(G_unified, \"Unified Graph\")\n",
    "            logger.info(\"[7/7] Creating unified rulebook integration...\")\n",
    "            self.visualize_rulebook_integration_graph(G_unified, \"Unified Graph\")\n",
    "        logger.info(\"\\nStep 3 Completed.\")\n",
    "        logger.info(f\"   Unified graph: {G_unified.number_of_nodes():,} nodes, {G_unified.number_of_edges():,} edges\")\n",
    "        if skip_large_visualizations and G_unified.number_of_nodes() > 10000:\n",
    "            logger.info(f\"   Generated 2 visualizations (limited for large graph)\")\n",
    "        else:\n",
    "            logger.info(f\"   Generated 7 comprehensive visualizations\")\n",
    "    def step4_export_for_gnn(self, output_dir: str = \"gnn_data\"):\n",
    "        logger.info(\"\\n\")\n",
    "        logger.info(\"Step 4: Exporting for GNN Processing -\")\n",
    "        if not self.global_knowledge_graph or self.global_knowledge_graph.number_of_nodes() == 0:\n",
    "            logger.error(\"No unified graph found.\")\n",
    "            return\n",
    "        self.export_for_gnn(output_dir=output_dir)\n",
    "        logger.info(\"\\nStep 4 Completed.\")\n",
    "        logger.info(f\"   GNN-ready datasets exported to '{output_dir}/' directory\")\n",
    "    def step5_generate_analysis_report(self):\n",
    "        logger.info(\"\\n\")\n",
    "        logger.info(\"Step 5: Generating Analysis Report -\")\n",
    "        if not self.global_knowledge_graph or self.global_knowledge_graph.number_of_nodes() == 0:\n",
    "            logger.error(\"No unified graph found.\")\n",
    "            return\n",
    "        self.generate_comprehensive_report()\n",
    "        logger.info(\"\\nStep 5 Completed.\")\n",
    "        logger.info(\"   Comprehensive analysis report generated\")\n",
    "        logger.info(\"\\nAll Steps Completed.\")\n",
    "if __name__ == \"__main__\":\n",
    "    OWL_PATH = \"IndiLegalOnt.owl\"\n",
    "    PROCESSED_DIR = \"dataset_processed\"\n",
    "    RULES_DIR = \"official_documents\"    \n",
    "    pipeline = LegalKnowledgeGraphPipeline(\n",
    "        owl_path=OWL_PATH, processed_dir=PROCESSED_DIR,\n",
    "        rules_dir=RULES_DIR, cache_dir=\"entity_cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 09:36:44,180 - INFO - \n",
      "\n",
      "2025-11-06 09:36:44,181 - INFO - Step 1: Setup and Data Loading -\n",
      "2025-11-06 09:36:44,182 - INFO - Loading official legal documents from PDFs...\n",
      "2025-11-06 09:36:44,184 - INFO - Loading Indian Penal Code.pdf...\n",
      "2025-11-06 09:36:44,335 - INFO -   Reading 119 pages...\n",
      "2025-11-06 09:36:46,920 - INFO - Loading Code of Criminal Procedure.pdf...\n",
      "2025-11-06 09:36:46,945 - INFO -   Reading 263 pages...\n",
      "2025-11-06 09:36:52,718 - INFO - Loading Constitution of India.pdf...\n",
      "2025-11-06 09:36:52,740 - INFO -   Reading 402 pages...\n",
      "2025-11-06 09:37:01,330 - INFO - Loading Indian Evidence Act.pdf...\n",
      "2025-11-06 09:37:01,334 - INFO -   Reading 60 pages...\n",
      "2025-11-06 09:37:02,713 - INFO - Indexed 1684 provisions\n",
      "2025-11-06 09:37:02,714 - INFO -   Indian Penal Code: 498 provisions\n",
      "2025-11-06 09:37:02,714 - INFO -   Code of Criminal Procedure: 493 provisions\n",
      "2025-11-06 09:37:02,714 - INFO -   Constitution of India: 497 provisions\n",
      "2025-11-06 09:37:02,714 - INFO -   Indian Evidence Act: 196 provisions\n",
      "2025-11-06 09:37:02,715 - INFO - Loading all case files...\n",
      "2025-11-06 09:37:02,832 - INFO - Loading 9979 cases from supreme_court...\n",
      "Loading supreme_court: 100%|| 9979/9979 [00:03<00:00, 3169.24it/s]\n",
      "2025-11-06 09:37:06,024 - INFO - Loaded 9979 cases from supreme_court\n",
      "2025-11-06 09:37:06,063 - INFO - Loading 8398 cases from allahabad_high_court...\n",
      "Loading allahabad_high_court: 100%|| 8398/8398 [00:02<00:00, 4188.95it/s]\n",
      "2025-11-06 09:37:08,069 - INFO - Loaded 8398 cases from allahabad_high_court\n",
      "2025-11-06 09:37:08,105 - INFO - Loading 9436 cases from bombay_high_court...\n",
      "Loading bombay_high_court: 100%|| 9436/9436 [00:02<00:00, 3636.28it/s]\n",
      "2025-11-06 09:37:10,702 - INFO - Loaded 9436 cases from bombay_high_court\n",
      "2025-11-06 09:37:10,882 - INFO - Loading 9584 cases from calcutta_high_court...\n",
      "Loading calcutta_high_court: 100%|| 9584/9584 [00:02<00:00, 4724.38it/s]\n",
      "2025-11-06 09:37:12,912 - INFO - Loaded 9584 cases from calcutta_high_court\n",
      "2025-11-06 09:37:12,951 - INFO - Loading 8821 cases from delhi_high_court...\n",
      "Loading delhi_high_court: 100%|| 8821/8821 [00:02<00:00, 3722.80it/s]\n",
      "2025-11-06 09:37:15,322 - INFO - Loaded 8821 cases from delhi_high_court\n",
      "2025-11-06 09:37:15,363 - INFO - Loading 9807 cases from madras_high_court...\n",
      "Loading madras_high_court: 100%|| 9807/9807 [00:02<00:00, 3390.84it/s]\n",
      "2025-11-06 09:37:18,257 - INFO - Loaded 9807 cases from madras_high_court\n",
      "2025-11-06 09:37:18,258 - INFO - Total cases loaded: 56025\n",
      "2025-11-06 09:37:18,267 - INFO - \n",
      "Step 1 Completed.\n",
      "2025-11-06 09:37:18,289 - INFO -    Loaded 56,025 cases from 6 courts\n",
      "2025-11-06 09:37:18,297 - INFO -    Indexed 1684 provisions from rulebooks\n"
     ]
    }
   ],
   "source": [
    "pipeline.step1_setup()\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.step2_create_individual_graphs(num_cases_per_court=10)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.step2_create_individual_graphs(num_cases_per_court=100)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.step3_create_unified_graph(max_cases_per_court=250, skip_large_visualizations=False)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.step3_create_unified_graph(max_cases_per_court=None, skip_large_visualizations=True)\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 10:15:35,981 - INFO - \n",
      "\n",
      "2025-11-06 10:15:35,983 - INFO - Step 4: Exporting for GNN Processing -\n",
      "2025-11-06 10:15:35,985 - INFO - Exporting data for GNN...\n",
      "2025-11-06 10:15:41,043 - INFO - Graph saved as knowledge_graph.gpickle\n",
      "2025-11-06 10:15:43,068 - INFO - PyTorch Geometric format exported\n",
      "2025-11-06 10:15:43,069 - INFO - GNN data exported to gnn_data\n",
      "2025-11-06 10:15:43,227 - INFO - \n",
      "Step 4 Completed.\n",
      "2025-11-06 10:15:43,228 - INFO -    GNN-ready datasets exported to 'gnn_data/' directory\n"
     ]
    }
   ],
   "source": [
    "pipeline.step4_export_for_gnn(output_dir=\"gnn_data\")\n",
    "_ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 10:15:52,893 - INFO - \n",
      "\n",
      "2025-11-06 10:15:52,896 - INFO - Step 5: Generating Analysis Report -\n",
      "2025-11-06 10:15:52,897 - INFO - \n",
      "\n",
      "2025-11-06 10:15:52,899 - INFO - Knowledge Graph Analysis Report:\n",
      "2025-11-06 10:15:52,900 - INFO - \n",
      "Overall Statistics:\n",
      "2025-11-06 10:15:52,901 - INFO - Total Cases: 56,025\n",
      "2025-11-06 10:15:52,901 - INFO - Cases By Court:\n",
      "2025-11-06 10:15:52,902 - INFO -   Allahabad High Court: 8,398\n",
      "2025-11-06 10:15:52,902 - INFO -   Bombay High Court: 9,436\n",
      "2025-11-06 10:15:52,903 - INFO -   Calcutta High Court: 9,584\n",
      "2025-11-06 10:15:52,904 - INFO -   Delhi High Court: 8,821\n",
      "2025-11-06 10:15:52,904 - INFO -   Madras High Court: 9,807\n",
      "2025-11-06 10:15:52,904 - INFO -   Supreme Court: 9,979\n",
      "2025-11-06 10:15:52,905 - INFO - Cases By Year (Top 10):\n",
      "2025-11-06 10:15:52,905 - INFO -   2013: 2,400\n",
      "2025-11-06 10:15:52,905 - INFO -   2010: 2,396\n",
      "2025-11-06 10:15:52,905 - INFO -   2003: 2,395\n",
      "2025-11-06 10:15:52,906 - INFO -   2023: 2,394\n",
      "2025-11-06 10:15:52,906 - INFO -   2021: 2,392\n",
      "2025-11-06 10:15:52,907 - INFO -   2022: 2,391\n",
      "2025-11-06 10:15:52,908 - INFO -   2005: 2,390\n",
      "2025-11-06 10:15:52,908 - INFO -   2004: 2,389\n",
      "2025-11-06 10:15:52,909 - INFO -   2008: 2,387\n",
      "2025-11-06 10:15:52,909 - INFO -   2011: 2,386\n",
      "2025-11-06 10:15:52,909 - INFO - \n",
      "Unified Knowledge Graph:\n",
      "2025-11-06 10:15:52,910 - INFO -   Total Nodes: 154,068\n",
      "2025-11-06 10:15:52,953 - INFO -   Total Edges: 725,563\n",
      "2025-11-06 10:15:53,003 - INFO -   Avg Degree: 9.42\n",
      "2025-11-06 10:15:53,003 - INFO - \n",
      "Node Type Distribution:\n",
      "2025-11-06 10:15:53,051 - INFO -   Act: 79,548\n",
      "2025-11-06 10:15:53,051 - INFO -   Case: 56,025\n",
      "2025-11-06 10:15:53,052 - INFO -   Provision: 17,691\n",
      "2025-11-06 10:15:53,052 - INFO -   Judge: 766\n",
      "2025-11-06 10:15:53,053 - INFO -   Concept: 16\n",
      "2025-11-06 10:15:53,053 - INFO -   Doctrine: 15\n",
      "2025-11-06 10:15:53,053 - INFO -   High Court: 5\n",
      "2025-11-06 10:15:53,054 - INFO -   Supreme Court: 1\n",
      "2025-11-06 10:15:53,054 - INFO -   Supreme Court Cases: 1\n",
      "2025-11-06 10:15:53,055 - INFO - \n",
      "Edge Type Distribution:\n",
      "2025-11-06 10:15:53,259 - INFO -   Cites Provision: 325,718\n",
      "2025-11-06 10:15:53,259 - INFO -   Governed By: 181,308\n",
      "2025-11-06 10:15:53,259 - INFO -   Involves Concept: 135,303\n",
      "2025-11-06 10:15:53,260 - INFO -   Adjudicated: 56,025\n",
      "2025-11-06 10:15:53,260 - INFO -   Applies Doctrine: 23,861\n",
      "2025-11-06 10:15:53,260 - INFO -   Decided By: 3,343\n",
      "2025-11-06 10:15:53,261 - INFO -   Superior To: 5\n",
      "2025-11-06 10:15:53,261 - INFO - \n",
      "Top Cited Acts:\n",
      "2025-11-06 10:15:53,329 - INFO -   Indian Penal Code: 2429 citations\n",
      "2025-11-06 10:15:53,330 - INFO -   Evidence Act: 2049 citations\n",
      "2025-11-06 10:15:53,330 - INFO -   A\n",
      "of the Act: 1724 citations\n",
      "2025-11-06 10:15:53,330 - INFO -   Income Tax Act, 1961: 1705 citations\n",
      "2025-11-06 10:15:53,330 - INFO -   Arbitration and Conciliation Act, 1996: 1672 citations\n",
      "2025-11-06 10:15:53,331 - INFO -   Limitation Act: 1346 citations\n",
      "2025-11-06 10:15:53,331 - INFO -   Civil Procedure Code: 1200 citations\n",
      "2025-11-06 10:15:53,332 - INFO -   Criminal Procedure Code: 1080 citations\n",
      "2025-11-06 10:15:53,333 - INFO -   Companies Act, 1956: 1073 citations\n",
      "2025-11-06 10:15:53,334 - INFO -   Income Tax Act: 1003 citations\n",
      "2025-11-06 10:15:53,334 - INFO - \n",
      "Top Cited Provisions:\n",
      "2025-11-06 10:15:53,379 - INFO -   i: 20496 citations\n",
      "2025-11-06 10:15:53,380 - INFO -   226: 9139 citations [Constitution of India]\n",
      "2025-11-06 10:15:53,380 - INFO -   4: 3783 citations [Indian Penal Code]\n",
      "2025-11-06 10:15:53,380 - INFO -   3: 3264 citations [Indian Penal Code]\n",
      "2025-11-06 10:15:53,381 - INFO -   14: 3260 citations [Constitution of India]\n",
      "2025-11-06 10:15:53,381 - INFO -   9: 3042 citations [Indian Penal Code]\n",
      "2025-11-06 10:15:53,381 - INFO -   34: 2909 citations [Indian Penal Code]\n",
      "2025-11-06 10:15:53,382 - INFO -   11: 2694 citations [Indian Penal Code]\n",
      "2025-11-06 10:15:53,382 - INFO -   6: 2656 citations [Indian Penal Code]\n",
      "2025-11-06 10:15:53,382 - INFO -   5: 2407 citations [Indian Penal Code]\n",
      "2025-11-06 10:15:53,382 - INFO - \n",
      "Rulebook Integration Statistics:\n",
      "2025-11-06 10:15:53,459 - INFO -   Total Provisions Extracted: 17691\n",
      "2025-11-06 10:15:53,459 - INFO - \n",
      "\n",
      "2025-11-06 10:15:53,461 - INFO - \n",
      "Step 5 Completed.\n",
      "2025-11-06 10:15:53,461 - INFO -    Comprehensive analysis report generated\n",
      "2025-11-06 10:15:53,461 - INFO - \n",
      "All Steps Completed.\n"
     ]
    }
   ],
   "source": [
    "pipeline.step5_generate_analysis_report()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
