{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 11:17:13,035 - INFO - Initializing Two-Stage Neurosymbolic Legal Retriever...\n",
      "2025-11-06 11:17:13,036 - INFO - Loading knowledge graph...\n",
      "2025-11-06 11:17:13,790 - INFO - Loaded graph: 154068 nodes, 725563 edges\n",
      "2025-11-06 11:17:13,790 - INFO - Loading GAT embeddings...\n",
      "2025-11-06 11:17:14,377 - INFO - Loaded GAT embeddings: (154068, 128)\n",
      "2025-11-06 11:17:14,377 - INFO - Loading case metadata...\n",
      "2025-11-06 11:17:14,378 - INFO - Loading cached case metadata...\n",
      "2025-11-06 11:17:31,883 - INFO - Loaded 56025 cases from cache\n",
      "2025-11-06 11:17:31,903 - INFO - Loading official legal documents...\n",
      "2025-11-06 11:17:31,905 - INFO - Loading cached PDF chunks...\n",
      "2025-11-06 11:17:31,908 - INFO - Initializing text encoder...\n",
      "2025-11-06 11:17:31,984 - INFO - Use pytorch device_name: mps\n",
      "2025-11-06 11:17:31,985 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-11-06 11:17:35,107 - INFO - Loaded: all-MiniLM-L6-v2 (384 dim)\n",
      "2025-11-06 11:17:35,108 - INFO - Building text embeddings...\n",
      "2025-11-06 11:17:35,108 - INFO - Loading cached text embeddings...\n",
      "2025-11-06 11:17:35,128 - INFO - Loaded cached embeddings: (56135, 384)\n",
      "2025-11-06 11:17:35,129 - INFO - Building case-to-node mapping...\n",
      "2025-11-06 11:17:35,129 - INFO - Loading cached case-to-node mapping...\n",
      "2025-11-06 11:17:35,163 - INFO - Initializing Legal Chatbot with LLM: deepseek-r1:7b\n",
      "2025-11-06 11:17:35,193 - INFO - HTTP Request: GET http://127.0.0.1:11434/api/tags \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 11:17:35,195 - INFO - Ollama connected.\n",
      "2025-11-06 11:17:35,195 - INFO - \n",
      "\n",
      "2025-11-06 11:17:35,195 - INFO - Processing Query...\n",
      "2025-11-06 11:17:35,196 - INFO - Query: What punishment is there for murder under IPC?\n",
      "\n",
      "2025-11-06 11:17:35,197 - INFO - Step 1/3: Two-stage neurosymbolic retrieval...\n",
      "2025-11-06 11:17:35,197 - INFO - Two-stage retrieval for query: 'What punishment is there for murder under IPC?...'\n",
      "2025-11-06 11:17:35,197 - INFO -   Stage 1: Text retrieval...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Example Query - Demonstrating Two-Stage Retrieval\n",
      "\n",
      "\n",
      "Query: What punishment is there for murder under IPC?\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5f0d6be1174170afccb3c419d0638a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-06 11:17:37,285 - INFO -   Stage 2: GAT + Symbolic re-ranking...\n",
      "2025-11-06 11:17:37,383 - INFO - Step 2/3: Building context...\n",
      "2025-11-06 11:17:37,383 - INFO - Step 3/3: Generating response with deepseek-r1:7b...\n",
      "2025-11-06 11:18:39,678 - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
      "2025-11-06 11:18:39,683 - INFO - Response generated successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ChatBot Response:\n",
      "\n",
      "\n",
      "**Comprehensive Legal Research Response: Punishment for Murder under IPC**\n",
      "\n",
      "The Indian Penal Code (IPC) Section 302 addresses murder and is divided into two parts:\n",
      "\n",
      "1. **Murder through an honest and lawful act (Section 302(1))**: This includes killing with malice aforethought or without premeditation but through a lawful act. The punishment typically includes life imprisonment without possibility of further penalty.\n",
      "\n",
      "2. **Premeditated Murder (Section 302(2)(a))**: This involves killing with intent to kill, usually resulting in shorter sentences such as 7 to 10 years imprisonment.\n",
      "\n",
      "**Analysis of Cases:**\n",
      "\n",
      "- **Case 1 (Anil Lala Saundade vs The State Of Maharashtra And Ors., 2003)**: The conviction received a sentence of actual imprisonment exceeding 14 years, aligning with life imprisonment under Section 302(1).\n",
      "\n",
      "- **Case 2 (Prem Singh vs State Of Himachal Pradesh, 2003)**: While the exact punishment wasn't specified, it likely followed severe penalties typical for IPC Section 302(1) cases.\n",
      "\n",
      "- **Case 3 (Kavinder And Ors. vs State (Nct Of Delhi), 2004)**: This case cited a judgment where life imprisonment was awarded under Section 302(1).\n",
      "\n",
      "- **Case 4 (Yasuddin, Nettai Ibrahim @ Ibrahim And ... vs Inspector Of Police, 2004)**: Section 307, which pertains to intent-to-kill, typically results in shorter sentences like 7 to 10 years under premeditated murder.\n",
      "\n",
      "- **Case 5 (Narender Kumar And Ors. vs State (Govt.) Of Nct Of Delhi [Along With ...], 2005)**: The sentence awarded was 7 years imprisonment without further penalty, consistent with Section 302(2)(a).\n",
      "\n",
      "**Key Legal Principles and Precedents:**\n",
      "\n",
      "- IPC Sections 302(1) and 302(2)(a) govern murder with varying sentences.\n",
      "- The strict liability nature of these sections ensures severe penalties regardless of intent.\n",
      "- The Supreme Court emphasized life imprisonment for non-premeditated murder under Section 302(1).\n",
      "\n",
      "**Court Hierarchy:**\n",
      "\n",
      "- The Supreme Court's judgment is binding on all lower courts, and its pronouncement in related cases underscores the harsher penalties.\n",
      "\n",
      "**Recommendation:**\n",
      "\n",
      "- ** severe punishment**: Murder under IPC typically results in life imprisonment without possibility of further penalty (Section 302(1)).\n",
      "- **Shorter Sentences for Premeditated Murder**: If murder is premeditated, imprisonment ranges from 7 to 10 years (Section 302(2)(a)).\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Murder under IPC can lead to severe penalties, with life imprisonment being the typical outcome. Premeditated murder may result in shorter sentences. Courts should adhere to these guidelines while determining sentencing.\n",
      "\n",
      "\n",
      "Retrieval Analysis:\n",
      "\n",
      "\n",
      "Stage 1: Retrieved 100 candidates via text search\n",
      "Stage 2: Re-ranked to top 5 using GAT + Symbolic\n",
      "\n",
      "Top 5 Retrieved Cases:\n",
      "\n",
      "1. Anil Lala Saundade vs The State Of Maharashtra And Ors. on 14 January,\n",
      "   Court: Bombay High Court | Date: 2003-01-14\n",
      "   Citation Network: Cited by 1 cases, cites 5 cases\n",
      "\n",
      "2. Prem Singh vs State Of Himachal Pradesh on 16 September, 2003\n",
      "   Court: Supreme Court | Date: 2003-09-16\n",
      "   Citation Network: Cited by 1 cases, cites 3 cases\n",
      "\n",
      "3. Kavinder And Ors. vs State (Nct Of Delhi) on 25 November, 2004\n",
      "   Court: Delhi High Court | Date: 2004-11-25\n",
      "   Citation Network: Cited by 1 cases, cites 23 cases\n",
      "\n",
      "4. Yasuddin, Nettai Ibrahim @ Ibrahim And ... vs Inspector Of Police on 1\n",
      "   Court: Madras High Court | Date: 2004-10-15\n",
      "   Citation Network: Cited by 1 cases, cites 4 cases\n",
      "\n",
      "5. Narender Kumar And Ors. vs State (Govt.) Of Nct Of Delhi [Along With .\n",
      "   Court: Delhi High Court | Date: 2005-04-26\n",
      "   Citation Network: Cited by 1 cases, cites 18 cases\n",
      "\n",
      "\n",
      "Example completed.\n",
      "\n",
      "To start interactive mode later, run:\n",
      "  chatbot.interactive_chat()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import logging\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "class NeurosymbolicLegalRetriever:\n",
    "    def __init__(self, gnn_data_dir: str = \"gnn_data\", \n",
    "                 processed_dir: str = \"dataset_processed\",\n",
    "                 rules_dir: str = \"official_documents\"):\n",
    "        self.gnn_data_dir = Path(gnn_data_dir)\n",
    "        self.processed_dir = Path(processed_dir)\n",
    "        self.rules_dir = Path(rules_dir)\n",
    "        logger.info(\"Initializing Two-Stage Neurosymbolic Legal Retriever...\")\n",
    "        self.cases = []\n",
    "        self.case_id_to_idx = {}        \n",
    "        self.load_knowledge_graph()\n",
    "        self.load_gat_embeddings()\n",
    "        self.load_case_metadata() \n",
    "        self.load_official_pdfs()  \n",
    "        self.initialize_text_encoder()\n",
    "        self.build_text_embeddings()\n",
    "        self.build_case_to_node_mapping()\n",
    "        case_count = sum(1 for c in self.cases if c.get('doc_type', 'case') == 'case')\n",
    "        provision_count = sum(1 for c in self.cases if c.get('doc_type') == 'provision')\n",
    "        pdf_count = sum(1 for c in self.cases if c.get('doc_type') == 'pdf_document')    \n",
    "    def load_knowledge_graph(self):\n",
    "        logger.info(\"Loading knowledge graph...\")\n",
    "        kg_path = self.gnn_data_dir / 'knowledge_graph.gpickle'\n",
    "        if not kg_path.exists():\n",
    "            logger.error(f\"Knowledge graph not found at {kg_path}\")\n",
    "            raise FileNotFoundError(f\"Missing: {kg_path}\")\n",
    "        with open(kg_path, 'rb') as f:\n",
    "            self.G = pickle.load(f)\n",
    "        logger.info(f\"Loaded graph: {self.G.number_of_nodes()} nodes, {self.G.number_of_edges()} edges\")\n",
    "    def load_gat_embeddings(self):\n",
    "        logger.info(\"Loading GAT embeddings...\")        \n",
    "        emb_path = self.gnn_data_dir / 'final_embeddings_node_type.npy'\n",
    "        if not emb_path.exists():\n",
    "            logger.warning(f\"GAT embeddings not found at {emb_path}\")\n",
    "            self.gat_embeddings = None\n",
    "            self.node_metadata = []\n",
    "            self.node_id_to_gat_idx = {}\n",
    "            return\n",
    "        self.gat_embeddings = np.load(emb_path)\n",
    "        with open(self.gnn_data_dir / 'node_metadata.json', 'r') as f:\n",
    "            self.node_metadata = json.load(f)        \n",
    "        self.node_id_to_gat_idx = {}\n",
    "        for idx, node_meta in enumerate(self.node_metadata):\n",
    "            node_id = node_meta['id']\n",
    "            self.node_id_to_gat_idx[node_id] = idx\n",
    "        logger.info(f\"Loaded GAT embeddings: {self.gat_embeddings.shape}\")\n",
    "    def load_case_metadata(self):\n",
    "        logger.info(\"Loading case metadata...\")    \n",
    "        cache_path = self.gnn_data_dir / 'cases_metadata_cache.json'    \n",
    "        if cache_path.exists():\n",
    "            logger.info(\"Loading cached case metadata...\")\n",
    "            try:\n",
    "                with open(cache_path, 'r', encoding='utf-8') as f:\n",
    "                    cache_data = json.load(f)\n",
    "                self.cases = cache_data['cases']\n",
    "                self.case_id_to_idx = cache_data['case_id_to_idx']\n",
    "                initial_case_count = len(self.cases)\n",
    "                logger.info(f\"Loaded {initial_case_count} cases from cache\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load cached case metadata: {e}\")\n",
    "        self.cases = []\n",
    "        self.case_id_to_idx = {}\n",
    "        courts = ['supreme_court', 'delhi_high_court', 'bombay_high_court',\n",
    "                  'calcutta_high_court', 'allahabad_high_court', 'madras_high_court']\n",
    "        for court in courts:\n",
    "            court_dir = self.processed_dir / court\n",
    "            if not court_dir.exists():\n",
    "                continue\n",
    "            json_files = list(court_dir.glob('*.json'))\n",
    "            for json_file in tqdm(json_files, desc=f\"Loading {court}\", leave=False):\n",
    "                try:\n",
    "                    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "                        case_data = json.load(f)\n",
    "                    case_id = json_file.stem\n",
    "                    idx = len(self.cases)\n",
    "                    self.cases.append({\n",
    "                        'id': case_id, 'file_name': case_data['file_name'],\n",
    "                        'court': court, 'metadata': case_data['metadata'],\n",
    "                        'text': case_data['text'], 'text_length': case_data.get('text_length', 0),\n",
    "                        'word_count': case_data.get('word_count', 0), 'doc_type': 'case'})\n",
    "                    self.case_id_to_idx[case_id] = idx\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error loading {json_file}: {e}\")\n",
    "        initial_case_count = len(self.cases)\n",
    "        logger.info(f\"Loaded {initial_case_count} cases\")\n",
    "        try:\n",
    "            cache_data = {\n",
    "                'cases': self.cases, 'case_id_to_idx': self.case_id_to_idx}\n",
    "            with open(cache_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(cache_data, f, ensure_ascii=False)\n",
    "            logger.info(f\"Cached case metadata to: {cache_path}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to cache metadata: {e}\")\n",
    "    def load_official_pdfs(self):\n",
    "        logger.info(\"Loading official legal documents...\")\n",
    "        cache_path = self.gnn_data_dir / 'pdf_chunks_cache.json'\n",
    "        if cache_path.exists():\n",
    "            logger.info(\"Loading cached PDF chunks...\")\n",
    "            try:\n",
    "                with open(cache_path, 'r', encoding='utf-8') as f:\n",
    "                    pdf_chunks = json.load(f)\n",
    "                total_chunks = 0\n",
    "                for chunk in pdf_chunks:\n",
    "                    self.cases.append(chunk)\n",
    "                    total_chunks += 1\n",
    "                return\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load cached PDF chunks: {e}\")\n",
    "        pdf_files = {\n",
    "            'ipc': 'Indian Penal Code.pdf', 'crpc': 'Code of Criminal Procedure.pdf',\n",
    "            'constitution': 'Constitution of India.pdf', 'evidence': 'Indian Evidence Act.pdf'}\n",
    "        try:\n",
    "            import PyPDF2\n",
    "        except ImportError:\n",
    "            logger.error(\"PyPDF2 not installed.\")\n",
    "            logger.warning(\"Skipping PDF loading. Official documents won't be searchable.\")\n",
    "            return\n",
    "        pdf_chunks_to_cache = []    \n",
    "        total_chunks = 0\n",
    "        for doc_type, filename in pdf_files.items():\n",
    "            filepath = self.rules_dir / filename\n",
    "            if not filepath.exists():\n",
    "                logger.warning(f\"PDF not found: {filepath}\")\n",
    "                continue\n",
    "            try:\n",
    "                logger.info(f\"  Reading {filename}...\")\n",
    "                with open(filepath, 'rb') as file:\n",
    "                    pdf_reader = PyPDF2.PdfReader(file)\n",
    "                    full_text = \"\"                    \n",
    "                    for page_num, page in enumerate(pdf_reader.pages):\n",
    "                        text = page.extract_text()\n",
    "                        if text:\n",
    "                            full_text += text + \"\\n\"\n",
    "                    if not full_text.strip():\n",
    "                        logger.warning(f\"  No text extracted from {filename}\")\n",
    "                        continue                    \n",
    "                    chunks = self._chunk_document(full_text, doc_type, chunk_size=1000)\n",
    "                    for i, chunk in enumerate(chunks):\n",
    "                        chunk_data = {\n",
    "                            'id': f\"{doc_type}_chunk_{i}\", 'file_name': f\"{filename}_chunk_{i}\",\n",
    "                            'court': 'statutory',\n",
    "                            'metadata': {\n",
    "                                'title': f\"{filename.replace('.pdf', '')} - Part {i+1}\",\n",
    "                                'date': 'Statutory','citations': [],\n",
    "                                'source_pdf': filename,'chunk_number': i+1},\n",
    "                            'text': chunk,'text_length': len(chunk),\n",
    "                            'word_count': len(chunk.split()),\n",
    "                            'doc_type': 'pdf_document'}\n",
    "                        self.cases.append(chunk_data)\n",
    "                        pdf_chunks_to_cache.append(chunk_data)         \n",
    "                        total_chunks += 1\n",
    "            except Exception as e:\n",
    "                logger.error(f\"  Error reading {filename}: {e}\")\n",
    "                continue\n",
    "        if pdf_chunks_to_cache:\n",
    "            try:\n",
    "                with open(cache_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(pdf_chunks_to_cache, f, ensure_ascii=False)\n",
    "                logger.info(f\"Cached {total_chunks} PDF chunks to: {cache_path}\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to cache PDF chunks: {e}\")\n",
    "    def _chunk_document(self, text: str, doc_type: str, chunk_size: int = 1000) -> List[str]:\n",
    "        chunks = []        \n",
    "        if doc_type == 'constitution':\n",
    "            parts = text.split('Article')\n",
    "            for i, part in enumerate(parts[1:], 1):\n",
    "                if len(part) > 50: \n",
    "                    chunk_text = f\"Article{part[:chunk_size]}\"\n",
    "                    chunks.append(chunk_text)\n",
    "        elif doc_type in ['ipc', 'crpc', 'evidence']:\n",
    "            parts = text.split('Section')\n",
    "            for i, part in enumerate(parts[1:], 1):\n",
    "                if len(part) > 50: \n",
    "                    chunk_text = f\"Section{part[:chunk_size]}\"\n",
    "                    chunks.append(chunk_text)        \n",
    "        if not chunks or len(chunks) < 5:\n",
    "            chunks = []\n",
    "            words = text.split()\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "            for word in words:\n",
    "                current_chunk.append(word)\n",
    "                current_length += len(word) + 1\n",
    "                if current_length >= chunk_size:\n",
    "                    chunks.append(' '.join(current_chunk))\n",
    "                    current_chunk = []\n",
    "                    current_length = 0\n",
    "            if current_chunk:\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "        if not chunks:\n",
    "            for i in range(0, len(text), chunk_size):\n",
    "                chunk = text[i:i+chunk_size]\n",
    "                if chunk.strip():\n",
    "                    chunks.append(chunk)\n",
    "        return chunks\n",
    "    def load_official_provisions(self):\n",
    "        if self.provisions_loaded:\n",
    "            logger.debug(\"Provisions already loaded, skipping...\")\n",
    "            return\n",
    "        logger.info(\"Loading provisions from official documents...\")        \n",
    "        initial_count = len(self.cases)\n",
    "        provision_index_path = self.gnn_data_dir / 'provision_index.json'\n",
    "        if not provision_index_path.exists():\n",
    "            logger.warning(\"Provision index not found. Trying to load from KG...\")\n",
    "            self.extract_provisions_from_graph()\n",
    "            self.provisions_loaded = True\n",
    "            return        \n",
    "        try:\n",
    "            with open(provision_index_path, 'r') as f:\n",
    "                provision_index = json.load(f)\n",
    "            provisions_added = 0\n",
    "            for doc_type, provisions in provision_index.items():\n",
    "                for prov_num, prov_data in provisions.items():\n",
    "                    idx = len(self.cases)                    \n",
    "                    provision_text = f\"{prov_data['title']}\\n\\n{prov_data['text']}\"\n",
    "                    self.cases.append({\n",
    "                        'id': f\"{doc_type}_{prov_num}\", 'file_name': f\"{doc_type}_{prov_num}\",\n",
    "                        'court': 'statutory',\n",
    "                        'metadata': {\n",
    "                            'title': f\"{prov_data['act']} - Section/Article {prov_num}\",\n",
    "                            'date': 'Statutory', 'citations': [],\n",
    "                            'provision_number': prov_num,\n",
    "                            'parent_act': prov_data['act']},\n",
    "                        'text': provision_text,\n",
    "                        'text_length': len(provision_text),\n",
    "                        'word_count': len(provision_text.split()),\n",
    "                        'doc_type': 'provision'})\n",
    "                    provisions_added += 1\n",
    "            self.provisions_loaded = True            \n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not load provision index: {e}\")\n",
    "            self.extract_provisions_from_graph()\n",
    "            self.provisions_loaded = True\n",
    "    def extract_provisions_from_graph(self):\n",
    "        if self.provisions_loaded:\n",
    "            logger.debug(\"Provisions already loaded, skipping extraction...\")\n",
    "            return\n",
    "        logger.info(\"Extracting provisions from knowledge graph...\")\n",
    "        initial_count = len(self.cases)\n",
    "        provisions_added = 0\n",
    "        for node_id, node_data in self.G.nodes(data=True):\n",
    "            if node_data.get('node_type') == 'provision':\n",
    "                label = node_data.get('label', '')\n",
    "                parent_act = node_data.get('parent_act', 'Unknown Act')\n",
    "                rulebook = node_data.get('rulebook', '')\n",
    "                provision_text = f\"{parent_act} - {label}\\n\\n[Provision text from {rulebook}]\"\n",
    "                self.cases.append({\n",
    "                    'id': f\"provision_{node_id}\",\n",
    "                    'file_name': node_id,'court': 'statutory',\n",
    "                    'metadata': {\n",
    "                        'title': f\"{parent_act} - {label}\",\n",
    "                        'date': 'Statutory', 'citations': [],\n",
    "                        'parent_act': parent_act},\n",
    "                    'text': provision_text, 'text_length': len(provision_text),\n",
    "                    'word_count': len(provision_text.split()),\n",
    "                    'doc_type': 'provision'})\n",
    "                provisions_added += 1\n",
    "        logger.info(f\"Extracted {provisions_added} provisions from graph (total docs: {len(self.cases)})\")\n",
    "        self.provisions_loaded = True        \n",
    "        self.load_official_provisions()\n",
    "    def initialize_text_encoder(self):\n",
    "        logger.info(\"Initializing text encoder...\")\n",
    "        try:\n",
    "            self.text_encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "            logger.info(\"Loaded: all-MiniLM-L6-v2 (384 dim)\")\n",
    "        except:\n",
    "            logger.warning(\"Could not load text encoder\")\n",
    "            self.text_encoder = None\n",
    "    def build_text_embeddings(self):\n",
    "        logger.info(\"Building text embeddings...\")\n",
    "        cache_path = self.gnn_data_dir / 'text_embeddings.npy'\n",
    "        if cache_path.exists():\n",
    "            logger.info(\"Loading cached text embeddings...\")\n",
    "            self.text_embeddings = np.load(cache_path)\n",
    "            logger.info(f\"Loaded cached embeddings: {self.text_embeddings.shape}\")\n",
    "            return\n",
    "        if self.text_encoder is None:\n",
    "            logger.warning(\"No text encoder available, skipping text embeddings\")\n",
    "            self.text_embeddings = None\n",
    "            return\n",
    "        texts = []\n",
    "        for case in self.cases:\n",
    "            text_snippet = case['text'][:512] if case['text'] else case['metadata'].get('title', '')\n",
    "            texts.append(text_snippet)        \n",
    "        batch_size = 32\n",
    "        embeddings = []\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Encoding texts\"):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            batch_emb = self.text_encoder.encode(batch, show_progress_bar=False)\n",
    "            embeddings.append(batch_emb)\n",
    "        self.text_embeddings = np.vstack(embeddings)\n",
    "        np.save(cache_path, self.text_embeddings)\n",
    "        logger.info(f\"Built text embeddings: {self.text_embeddings.shape}\")\n",
    "        logger.info(f\"Cached to: {cache_path}\")\n",
    "    def build_case_to_node_mapping(self):\n",
    "        logger.info(\"Building case-to-node mapping...\")\n",
    "        cache_path = self.gnn_data_dir / 'case_to_node_mapping.json'\n",
    "        if cache_path.exists():\n",
    "            logger.info(\"Loading cached case-to-node mapping...\")\n",
    "            try:\n",
    "                with open(cache_path, 'r') as f:\n",
    "                    cached_mapping = json.load(f)\n",
    "                    self.case_idx_to_node_id = {int(k): v for k, v in cached_mapping.items()}\n",
    "                return\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load cached mapping: {e}\")\n",
    "        self.case_idx_to_node_id = {}\n",
    "        for idx, case in enumerate(self.cases):\n",
    "            if case.get('doc_type') == 'provision':\n",
    "                parent_act = case['metadata'].get('parent_act', '')\n",
    "                prov_num = case['metadata'].get('provision_number', '')                \n",
    "                for node in self.G.nodes():\n",
    "                    node_data = self.G.nodes[node]\n",
    "                    if node_data.get('node_type') == 'provision':\n",
    "                        node_label = node_data.get('label', '')\n",
    "                        node_act = node_data.get('parent_act', '')\n",
    "                        if prov_num and str(prov_num) in node_label and parent_act in node_act:\n",
    "                            self.case_idx_to_node_id[idx] = node\n",
    "                            break\n",
    "                continue\n",
    "            node_id_1 = f\"Case_{idx+1}\"\n",
    "            court = case['court']\n",
    "            file_name = case['file_name']\n",
    "            if node_id_1 in self.G.nodes:\n",
    "                self.case_idx_to_node_id[idx] = node_id_1\n",
    "            else:\n",
    "                for node in self.G.nodes():\n",
    "                    node_data = self.G.nodes[node]\n",
    "                    if node_data.get('case_id') == case['id']:\n",
    "                        self.case_idx_to_node_id[idx] = node\n",
    "                        break        \n",
    "        case_count = sum(1 for c in self.cases if c.get('doc_type') == 'case')\n",
    "        provision_count = sum(1 for c in self.cases if c.get('doc_type') == 'provision')\n",
    "        mapped_count = len(self.case_idx_to_node_id)\n",
    "        logger.info(f\"Mapped {mapped_count} documents to graph nodes\")  \n",
    "        try:\n",
    "            with open(cache_path, 'w') as f:\n",
    "                json.dump({str(k): v for k, v in self.case_idx_to_node_id.items()}, f)\n",
    "            logger.info(f\"Cached mapping to: {cache_path}\")\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to cache mapping: {e}\") \n",
    "    def get_symbolic_scores(self, case_indices: List[int]) -> np.ndarray:\n",
    "        scores = np.zeros(len(case_indices))\n",
    "        for i, case_idx in enumerate(case_indices):\n",
    "            case = self.cases[case_idx]            \n",
    "            if case.get('doc_type') == 'pdf_document':\n",
    "                scores[i] = 0.3 \n",
    "                continue\n",
    "            node_id = self.case_idx_to_node_id.get(case_idx)            \n",
    "            if node_id and node_id in self.G.nodes:\n",
    "                node_data = self.G.nodes[node_id]                \n",
    "                pagerank = node_data.get('pagerank', 0)\n",
    "                pagerank_normalized = min(pagerank * 100, 1.0)\n",
    "                court = case['court']\n",
    "                if 'supreme_court' in court.lower():\n",
    "                    court_score = 1.0\n",
    "                elif 'high_court' in court.lower():\n",
    "                    court_score = 0.6\n",
    "                else:\n",
    "                    court_score = 0.3\n",
    "                cited_by = node_data.get('cited_by_count', 0)\n",
    "                citation_score = min(cited_by / 10, 1.0)                \n",
    "                year = node_data.get('year', '2000')\n",
    "                try:\n",
    "                    year_int = int(year)\n",
    "                    recency_score = max(0, min(1, (year_int - 2000) / 25))\n",
    "                except:\n",
    "                    recency_score = 0.3                 \n",
    "                symbolic_score = (\n",
    "                    pagerank_normalized * 0.25 + court_score * 0.35 +\n",
    "                    citation_score * 0.25 + recency_score * 0.15)\n",
    "            else:\n",
    "                court = case['court']\n",
    "                if 'supreme_court' in court.lower():\n",
    "                    symbolic_score = 0.8  \n",
    "                elif 'high_court' in court.lower():\n",
    "                    symbolic_score = 0.5 \n",
    "                else:\n",
    "                    symbolic_score = 0.3 \n",
    "            scores[i] = symbolic_score\n",
    "        return scores\n",
    "    def get_gat_context_scores(self, case_indices: List[int]) -> np.ndarray:\n",
    "        if self.gat_embeddings is None:\n",
    "            return np.zeros(len(case_indices))\n",
    "        scores = np.zeros(len(case_indices))        \n",
    "        context_embeddings = []\n",
    "        valid_indices = []\n",
    "        for case_idx in case_indices:\n",
    "            node_id = self.case_idx_to_node_id.get(case_idx)\n",
    "            if node_id and node_id in self.G.nodes and node_id in self.node_id_to_gat_idx:\n",
    "                gat_idx = self.node_id_to_gat_idx[node_id]\n",
    "                case_emb = self.gat_embeddings[gat_idx]                \n",
    "                try:\n",
    "                    cited_nodes = list(self.G.successors(node_id))\n",
    "                    citing_nodes = list(self.G.predecessors(node_id))                    \n",
    "                    neighbor_embs = [case_emb]                    \n",
    "                    for neighbor in cited_nodes + citing_nodes:\n",
    "                        if neighbor in self.node_id_to_gat_idx:\n",
    "                            neighbor_gat_idx = self.node_id_to_gat_idx[neighbor]\n",
    "                            neighbor_embs.append(self.gat_embeddings[neighbor_gat_idx])                    \n",
    "                    if len(neighbor_embs) > 1:\n",
    "                        weights = [2.0] + [1.0] * (len(neighbor_embs) - 1)\n",
    "                        context_emb = np.average(neighbor_embs, axis=0, weights=weights)\n",
    "                    else:\n",
    "                        context_emb = case_emb\n",
    "                    context_embeddings.append(context_emb)\n",
    "                    valid_indices.append(len(context_embeddings) - 1)\n",
    "                except:\n",
    "                    context_embeddings.append(case_emb)\n",
    "                    valid_indices.append(len(context_embeddings) - 1)\n",
    "            else:\n",
    "                context_embeddings.append(np.zeros(self.gat_embeddings.shape[1]))\n",
    "                valid_indices.append(len(context_embeddings) - 1)\n",
    "        context_embeddings = np.array(context_embeddings)        \n",
    "        if len(context_embeddings) > 1:\n",
    "            valid_contexts = context_embeddings[context_embeddings.sum(axis=1) != 0]\n",
    "            if len(valid_contexts) > 0:\n",
    "                centroid = np.mean(valid_contexts, axis=0)                \n",
    "                for i, context_emb in enumerate(context_embeddings):\n",
    "                    if context_emb.sum() != 0:\n",
    "                        scores[i] = cosine_similarity([context_emb], [centroid])[0][0]\n",
    "        return scores\n",
    "    def retrieve(self, query: str, top_k: int = 10, \n",
    "                stage1_k: int = 100,\n",
    "                alpha_text: float = 0.7, \n",
    "                alpha_gat: float = 0.15, \n",
    "                alpha_symbolic: float = 0.15) -> List[Dict]:\n",
    "        logger.info(f\"Two-stage retrieval for query: '{query[:50]}...'\")        \n",
    "        logger.info(f\"  Stage 1: Text retrieval...\")\n",
    "        if self.text_embeddings is not None and self.text_encoder is not None:\n",
    "            query_emb = self.text_encoder.encode([query])[0]\n",
    "            text_scores = cosine_similarity([query_emb], self.text_embeddings)[0]\n",
    "        else:\n",
    "            logger.error(\"Text embeddings not available\")\n",
    "            return []\n",
    "        stage1_indices = np.argsort(text_scores)[-stage1_k:][::-1]\n",
    "        stage1_text_scores = text_scores[stage1_indices]\n",
    "        logger.info(f\"  Stage 2: GAT + Symbolic re-ranking...\")        \n",
    "        gat_scores = self.get_gat_context_scores(stage1_indices.tolist())\n",
    "        symbolic_scores = self.get_symbolic_scores(stage1_indices.tolist())\n",
    "        def normalize(scores):\n",
    "            min_s, max_s = scores.min(), scores.max()\n",
    "            if max_s - min_s < 1e-8:\n",
    "                return np.ones_like(scores) * 0.5\n",
    "            return (scores - min_s) / (max_s - min_s)\n",
    "        text_scores_norm = normalize(stage1_text_scores)\n",
    "        gat_scores_norm = normalize(gat_scores)\n",
    "        symbolic_scores_norm = normalize(symbolic_scores)        \n",
    "        hybrid_scores = (\n",
    "            alpha_text * text_scores_norm + alpha_gat * gat_scores_norm + \n",
    "            alpha_symbolic * symbolic_scores_norm)        \n",
    "        top_reranked_indices = np.argsort(hybrid_scores)[-top_k:][::-1]\n",
    "        final_indices = stage1_indices[top_reranked_indices]\n",
    "        results = []\n",
    "        for rank, idx in enumerate(final_indices):\n",
    "            case = self.cases[idx]\n",
    "            reranked_idx = top_reranked_indices[rank]\n",
    "            node_id = self.case_idx_to_node_id.get(idx, None)            \n",
    "            neighbors_info = \"\"\n",
    "            if node_id and node_id in self.G.nodes:\n",
    "                try:\n",
    "                    citing_count = self.G.in_degree(node_id)\n",
    "                    cited_count = self.G.out_degree(node_id)\n",
    "                    neighbors_info = f\"Cited by {citing_count} cases, cites {cited_count} cases\"\n",
    "                except:\n",
    "                    pass\n",
    "            results.append({\n",
    "                'rank': rank + 1, 'case_id': case['id'],\n",
    "                'file_name': case['file_name'],'court': case['court'],\n",
    "                'title': case['metadata'].get('title', 'Unknown'),\n",
    "                'date': case['metadata'].get('date', 'Unknown'),\n",
    "                'text_snippet': case['text'][:500] if case['text'] else '',\n",
    "                'score': float(hybrid_scores[reranked_idx]),\n",
    "                'text_score': float(text_scores_norm[reranked_idx]),\n",
    "                'gat_score': float(gat_scores_norm[reranked_idx]),\n",
    "                'symbolic_score': float(symbolic_scores_norm[reranked_idx]),\n",
    "                'node_id': node_id,'neighbors_info': neighbors_info,\n",
    "                'citations': case['metadata'].get('citations', [])[:5],\n",
    "                'word_count': case['word_count']})\n",
    "        avg_text = np.mean([r['text_score'] for r in results])\n",
    "        avg_gat = np.mean([r['gat_score'] for r in results])\n",
    "        avg_symbolic = np.mean([r['symbolic_score'] for r in results])\n",
    "        return results\n",
    "class LegalChatbot:    \n",
    "    def __init__(self, retriever: NeurosymbolicLegalRetriever, \n",
    "                 llm_model: str = \"deepseek-r1:7b\"):\n",
    "        self.retriever = retriever\n",
    "        self.llm_model = llm_model\n",
    "        logger.info(f\"Initializing Legal Chatbot with LLM: {llm_model}\")\n",
    "        try:\n",
    "            models = ollama.list()\n",
    "            logger.info(f\"Ollama connected.\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Ollama connection failed: {e}\")\n",
    "            raise    \n",
    "    def format_context(self, retrieved_cases: List[Dict]) -> str:\n",
    "        context = \"\"        \n",
    "        for i, case in enumerate(retrieved_cases, 1):\n",
    "            doc_type = case.get('doc_type', 'case')\n",
    "            if doc_type == 'provision':\n",
    "                context += f\"\\n\"\n",
    "                context += f\"Statuary Provision {i}: {case['title']}\\n\"\n",
    "                context += f\"\\n\"\n",
    "                context += f\"Source: {case['metadata'].get('parent_act', 'Unknown Act')}\\n\"\n",
    "                context += f\"Type: Statutory Law\\n\"\n",
    "                context += f\"Relevance Score: {case['score']:.3f}\\n\"\n",
    "                context += f\"  └─ Text Similarity: {case['text_score']:.3f}\\n\"\n",
    "                context += f\"  └─ Graph Context (GAT): {case['gat_score']:.3f}\\n\"\n",
    "                context += f\"  └─ Citation Importance: {case['symbolic_score']:.3f}\\n\"\n",
    "                if case['neighbors_info']:\n",
    "                    context += f\"Usage in Cases: {case['neighbors_info']}\\n\"\n",
    "                context += f\"\\nProvision Text:\\n{case['text_snippet'][:600]}...\\n\"\n",
    "            elif doc_type == 'pdf_document':\n",
    "                context += f\"\\n\"\n",
    "                context += f\"Statuary Text {i}: {case['title']}\\n\"\n",
    "                context += f\"\\n\"\n",
    "                context += f\"Source: {case['metadata'].get('source_pdf', 'Unknown')}\\n\"\n",
    "                context += f\"Type: Official Legal Document\\n\"\n",
    "                context += f\"Relevance Score: {case['score']:.3f}\\n\"\n",
    "                context += f\"  └─ Text Similarity: {case['text_score']:.3f}\\n\"\n",
    "                context += f\"\\nExcerpt:\\n{case['text_snippet'][:700]}...\\n\"\n",
    "            else:\n",
    "                context += f\"\\n\"\n",
    "                context += f\"Case {i}: {case['title']}\\n\"\n",
    "                context += f\"\\n\"\n",
    "                context += f\"Court: {case['court'].replace('_', ' ').title()}\\n\"\n",
    "                context += f\"Date: {case['date']}\\n\"\n",
    "                context += f\"Overall Relevance: {case['score']:.3f}\\n\"\n",
    "                context += f\"  └─ Text Similarity: {case['text_score']:.3f}\\n\"\n",
    "                context += f\"  └─ Graph Context (GAT): {case['gat_score']:.3f}\\n\"\n",
    "                context += f\"  └─ Citation Importance: {case['symbolic_score']:.3f}\\n\"\n",
    "                if case['neighbors_info']:\n",
    "                    context += f\"Citation Network: {case['neighbors_info']}\\n\"\n",
    "                if case['citations']:\n",
    "                    context += f\"\\nKey Citations:\\n\"\n",
    "                    for cite in case['citations'][:3]:\n",
    "                        context += f\"  • {cite[:80]}\\n\"\n",
    "                context += f\"\\nSummary:\\n{case['text_snippet'][:400]}...\\n\"\n",
    "        return context\n",
    "    def chat(self, query: str, top_k: int = 5, \n",
    "            stage1_k: int = 100, alpha_text: float = 0.7,\n",
    "            alpha_gat: float = 0.15, alpha_symbolic: float = 0.15,\n",
    "            return_thinking: bool = True) -> Dict:\n",
    "        logger.info(f\"\\n\")\n",
    "        logger.info(f\"Processing Query...\")\n",
    "        logger.info(f\"Query: {query}\\n\")        \n",
    "        logger.info(\"Step 1/3: Two-stage neurosymbolic retrieval...\")\n",
    "        retrieved_cases = self.retriever.retrieve(\n",
    "            query, top_k=top_k,\n",
    "            stage1_k=stage1_k, alpha_text=alpha_text,\n",
    "            alpha_gat=alpha_gat, alpha_symbolic=alpha_symbolic)        \n",
    "        logger.info(\"Step 2/3: Building context...\")\n",
    "        context = self.format_context(retrieved_cases)        \n",
    "        logger.info(f\"Step 3/3: Generating response with {self.llm_model}...\")\n",
    "        prompt = f\"\"\"You are an expert Indian legal research assistant with deep knowledge of case law, statutes, and legal precedents.\n",
    "\n",
    "USER QUERY:\n",
    "{query}\n",
    "\n",
    "RETRIEVED CASES (Two-Stage Neurosymbolic Retrieval):\n",
    "These cases were retrieved using a sophisticated two-stage approach:\n",
    "\n",
    "STAGE 1 (Text Search): Semantic similarity identified {stage1_k} candidates using SBERT embeddings\n",
    "  → Searches case law, statutory provisions, AND official legal documents (IPC, CrPC, Constitution, Evidence Act)\n",
    "STAGE 2 (Re-ranking): The top {top_k} results were selected by combining:\n",
    "  • Text Similarity ({alpha_text*100:.0f}%): Semantic relevance to your query\n",
    "  • Graph Context ({alpha_gat*100:.0f}%): GAT embeddings capturing citation network structure\n",
    "  • Citation Importance ({alpha_symbolic*100:.0f}%): PageRank, court hierarchy, and citation counts\n",
    "\n",
    "NOTE: Results may include case law (judicial decisions), statutory provisions (from graph), and direct excerpts from official legal documents (IPC, CrPC, Constitution, Evidence Act PDFs).\n",
    "\n",
    "{context}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "Provide a comprehensive legal research response:\n",
    "\n",
    "1. **Direct Answer**: Address the query based on the retrieved cases\n",
    "\n",
    "2. **Case Analysis**: \n",
    "   - Identify the most relevant case(s) and explain why\n",
    "   - Note how the GAT graph context scores influenced ranking\n",
    "   - Highlight key legal principles and precedents\n",
    "   - Discuss any conflicting interpretations\n",
    "\n",
    "3. **Citation Network Context**:\n",
    "   - Consider how cases cite each other (shown in \"Citation Network\" info)\n",
    "   - Explain precedent relationships\n",
    "   - Note which cases are more influential (higher citation counts)\n",
    "\n",
    "4. **Court Hierarchy**:\n",
    "   - Supreme Court decisions are binding on all lower courts\n",
    "   - High Court decisions are persuasive authority\n",
    "   - Explain which court's decision carries more weight\n",
    "\n",
    "5. **Recommendation**:\n",
    "   - Provide actionable guidance based on case law\n",
    "   - Note any caveats or limitations\n",
    "\n",
    "**Be specific, cite case names, and explain your legal reasoning clearly.**\n",
    "\"\"\"\n",
    "        try:\n",
    "            response = ollama.generate(\n",
    "                model=self.llm_model, prompt=prompt,\n",
    "                options={\n",
    "                    'temperature': 0.7, 'num_ctx': 8192})\n",
    "            response_text = response['response']            \n",
    "            thinking = None\n",
    "            if return_thinking and '<thinking>' in response_text:\n",
    "                import re\n",
    "                thinking_match = re.search(r'<thinking>(.*?)</thinking>', \n",
    "                                          response_text, re.DOTALL)\n",
    "                if thinking_match:\n",
    "                    thinking = thinking_match.group(1).strip()\n",
    "                    response_text = re.sub(r'<thinking>.*?</thinking>', '', \n",
    "                                          response_text, flags=re.DOTALL).strip()            \n",
    "            logger.info(\"Response generated successfully.\")\n",
    "            return {\n",
    "                'query': query,'response': response_text,\n",
    "                'thinking': thinking, 'retrieved_cases': retrieved_cases,\n",
    "                'context': context,\n",
    "                'retrieval_config': {\n",
    "                    'stage1_candidates': stage1_k,\n",
    "                    'final_top_k': top_k,\n",
    "                    'weights': {\n",
    "                        'text': alpha_text,\n",
    "                        'gat': alpha_gat,'symbolic': alpha_symbolic}}}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"LLM generation failed: {e}\")\n",
    "            return {\n",
    "                'query': query,\n",
    "                'response': f\"Error generating response: {e}\",\n",
    "                'thinking': None, 'retrieved_cases': retrieved_cases,\n",
    "                'context': context}\n",
    "    def interactive_chat(self):\n",
    "        print(\"\\n\")\n",
    "        print(\"Indian Legal Case Recommender Chatbot :\")\n",
    "        print(\"Two-Stage Neurosymbolic Retrieval + DeepSeek-R1 -\")\n",
    "        print(\"\\nRetrieval: Text (70%) + GAT Context (15%) + Symbolic (15%) :-\")\n",
    "        print(\"Enter your legal query...\")\n",
    "        print(\"Type 'quit' to exit, 'help' for commands\\n\")\n",
    "        while True:\n",
    "            try:\n",
    "                query = input(\"You: \").strip()\n",
    "                if not query:\n",
    "                    continue\n",
    "                if query.lower() in ['quit', 'exit', 'q']:\n",
    "                    print(\"\\nGoodbye.\")\n",
    "                    break\n",
    "                if query.lower() == 'help':\n",
    "                    print(\"\\nCommands:\")\n",
    "                    print(\"  - Type your legal query naturally\")\n",
    "                    print(\"  - 'quit' or 'exit' - Exit the chatbot\")\n",
    "                    print(\"  - 'help' - Show this message\")\n",
    "                    print()\n",
    "                    continue\n",
    "                result = self.chat(\n",
    "                    query, top_k=5,\n",
    "                    stage1_k=100, alpha_text=0.7,\n",
    "                    alpha_gat=0.15, alpha_symbolic=0.15)                \n",
    "                print(\"\\n\")\n",
    "                print(\"Assistant:\\n\")\n",
    "                print(result['response'])\n",
    "                if result['thinking']:\n",
    "                    print(\"\\n\")\n",
    "                    print(\"Reasoning Process:\\n\")\n",
    "                    print(result['thinking'][:500] + \"...\" if len(result['thinking']) > 500 else result['thinking'])\n",
    "                print(\"\\n\")\n",
    "                print(f\"Retrieved Cases (Two-Stage Retrieval):\")\n",
    "                print(f\"    Stage 1: {result['retrieval_config']['stage1_candidates']} text candidates\")\n",
    "                print(f\"    Stage 2: Re-ranked to top {result['retrieval_config']['final_top_k']}\")\n",
    "                print(f\"\\n    Top 3 Results:\")\n",
    "                for i, case in enumerate(result['retrieved_cases'][:3], 1):\n",
    "                    print(f\"\\n  {i}. {case['title'][:70]}\")\n",
    "                    print(f\"     Court: {case['court'].replace('_', ' ').title()} | Date: {case['date']}\")\n",
    "                    print(f\"     Overall: {case['score']:.3f} (Text: {case['text_score']:.3f}, GAT: {case['gat_score']:.3f}, Symbolic: {case['symbolic_score']:.3f})\")\n",
    "                    if case['neighbors_info']:\n",
    "                        print(f\"     {case['neighbors_info']}\")\n",
    "                print(\"\\n\")\n",
    "            except KeyboardInterrupt:\n",
    "                print(\"\\nGoodbye.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError: {e}\\n\")\n",
    "if __name__ == \"__main__\":    \n",
    "    try:\n",
    "        retriever = NeurosymbolicLegalRetriever(\n",
    "            gnn_data_dir=\"gnn_data\", processed_dir=\"dataset_processed\",\n",
    "            rules_dir=\"official_documents\")\n",
    "        chatbot = LegalChatbot(\n",
    "            retriever=retriever, llm_model=\"deepseek-r1:7b\")\n",
    "        print(\"\\n\")\n",
    "        print(\"Example Query - Demonstrating Two-Stage Retrieval\")\n",
    "        print(\"\\n\")\n",
    "        example_query = \"What punishment is there for murder under IPC?\"\n",
    "        print(f\"Query: {example_query}\\n\")\n",
    "        result = chatbot.chat(\n",
    "            example_query, top_k=5,\n",
    "            stage1_k=100, alpha_text=0.7,\n",
    "            alpha_gat=0.15, alpha_symbolic=0.15)\n",
    "        print(\"\\n\")\n",
    "        print(\"ChatBot Response:\")\n",
    "        print(\"\\n\")\n",
    "        print(result['response'])\n",
    "        if result['thinking']:\n",
    "            print(\"\\n\")\n",
    "            print(\"Reasoning Process:\")\n",
    "            print(\"\\n\")\n",
    "            thinking_preview = result['thinking'][:600]\n",
    "            print(thinking_preview + \"...\" if len(result['thinking']) > 600 else thinking_preview)\n",
    "        print(\"\\n\")\n",
    "        print(\"Retrieval Analysis:\")\n",
    "        print(\"\\n\")\n",
    "        print(f\"Stage 1: Retrieved {result['retrieval_config']['stage1_candidates']} candidates via text search\")\n",
    "        print(f\"Stage 2: Re-ranked to top {result['retrieval_config']['final_top_k']} using GAT + Symbolic\")       \n",
    "        print(f\"\\nTop 5 Retrieved Cases:\")\n",
    "        for case in result['retrieved_cases']:\n",
    "            print(f\"\\n{case['rank']}. {case['title'][:70]}\")\n",
    "            print(f\"   Court: {case['court'].replace('_', ' ').title()} | Date: {case['date']}\")\n",
    "            if case['neighbors_info']:\n",
    "                print(f\"   Citation Network: {case['neighbors_info']}\")\n",
    "        print(\"\\n\")\n",
    "        print(\"Example completed.\")\n",
    "        response = input(\"Start interactive chat mode? (y/n): \").strip().lower()\n",
    "        if response in ['y', 'yes']:\n",
    "            print(\"\\n\")\n",
    "            print(\"Starting Interactive Mode...\")\n",
    "            chatbot.interactive_chat()\n",
    "        else:\n",
    "            print(\"\\nTo start interactive mode later, run:\")\n",
    "            print(\"  chatbot.interactive_chat()\\n\")        \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"\\nSetup incomplete: {e}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"\\nError: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.interactive_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
